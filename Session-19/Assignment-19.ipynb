{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "GVY3XJhuhvkg",
    "outputId": "3344865b-7c31-4282-d6cb-882af5de3e2e"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    os.chdir('/content')\n",
    "    if not os.path.isdir('/content/EVA-2-Group/'):\n",
    "        !git clone https://github.com/sambitdash/EVA-2-Group.git\n",
    "    os.chdir('/content/EVA-2-Group/Session-19')\n",
    "    !pwd\n",
    "    \n",
    "    !git config user.email \"sambitdash@gmail.com\"\n",
    "    !git config user.name \"Sambit Kumar Dash\"\n",
    "    !git config user.password \"your password\"\n",
    "    !git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "BbhSFX9Jhvkl",
    "outputId": "c4a40efa-9706-4279-f048-5bebb513c149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(332, 64, 64, 3)\n",
      "(410, 64, 64, 3)\n",
      "(337, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "car_types = ['hatch', 'sedan', 'suv']\n",
    "\n",
    "def resize_image(img, size=(64,64)):\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    if h == w: \n",
    "        return cv2.resize(img, size, cv2.INTER_AREA)\n",
    "\n",
    "    dif = h if h > w else w\n",
    "\n",
    "    interpolation = cv2.INTER_AREA if dif > (size[0]+size[1])//2 else cv2.INTER_CUBIC\n",
    "\n",
    "    x_pos = (dif - w)//2\n",
    "    y_pos = (dif - h)//2\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        mask = np.zeros((dif, dif), dtype=img.dtype)\n",
    "        mask[y_pos:y_pos+h, x_pos:x_pos+w] = img[:h, :w]\n",
    "    else:\n",
    "        c = img.shape[2]\n",
    "        mask = np.zeros((dif, dif, c), dtype=img.dtype)\n",
    "        mask[y_pos:y_pos+h, x_pos:x_pos+w, :] = img[:h, :w, :]\n",
    "\n",
    "    return cv2.resize(mask, size, interpolation)\n",
    "\n",
    "spath, dpath = join('data', 'cars'), join('data', 'norm')\n",
    "\n",
    "if not os.path.isdir(dpath):\n",
    "    os.mkdir(dpath)\n",
    "\n",
    "imgs = {}\n",
    "\n",
    "\n",
    "for ct in car_types:\n",
    "    sp, dp = join(spath, ct), join(dpath, ct)\n",
    "    alen = 1024\n",
    "    imgs[ct] = np.zeros((1024, 64, 64, 3))\n",
    "    if not os.path.isdir(dp):\n",
    "        os.mkdir(dp)\n",
    "    tlen = 0\n",
    "    for f in listdir(sp):\n",
    "        sf, df = join(sp, f), join(dp, f)\n",
    "        img = cv2.imread(sf)\n",
    "        img = resize_image(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        tlen += 1\n",
    "        if tlen > alen:\n",
    "            imgs[ct] = np.append(imgs[ct], np.zeros((1024, 64, 64, 3)))\n",
    "            alen += 1024\n",
    "        imgs[ct][tlen-1] = img\n",
    "    imgs[ct] = imgs[ct][:tlen]\n",
    "    print(imgs[ct].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "xgWY9ontMgFW",
    "outputId": "1b1466f1-116e-441b-9ccf-d19252b1719e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n",
      "(675, 64, 64, 3) y (675,)\n",
      "(67, 64, 64, 3) y (67,)\n",
      "(675, 64, 64, 3) y (675,)\n",
      "(67, 64, 64, 3) y (67,)\n",
      "[0.35301414 0.34561545 0.34175763] [0.3740168  0.37273017 0.37292084]\n",
      "-0.9438457 1.7650994\n",
      "-0.9438457 1.7650994\n",
      "(675, 64, 64, 3) (675,)\n"
     ]
    }
   ],
   "source": [
    "#%tensorflow_version 1.x\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)\n",
    "\n",
    "from tensorflow.keras import utils \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "trainx, testx = imgs['hatch'][:300,:,:,:], imgs['hatch'][300:,:,:,:]\n",
    "\n",
    "trainy, testy = np.zeros(trainx.shape[0], dtype=float), np.zeros(testx.shape[0], dtype=float)\n",
    "\n",
    "trainx = np.append(trainx, imgs['sedan'][:375,:,:,:], axis=0)\n",
    "testx = np.append(testx, imgs['sedan'][375:,:,:,:], axis=0)\n",
    "\n",
    "ltrain, ltest = trainx.shape[0] - trainy.shape[0], testx.shape[0] - testy.shape[0]\n",
    "\n",
    "trainy, testy = np.append(trainy, np.ones(ltrain, dtype=float)), np.append(testy, np.ones(ltest, dtype=float))\n",
    "\n",
    "\n",
    "print(trainx.shape, \"y\", trainy.shape)\n",
    "print(testx.shape, 'y', testy.shape)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "datagen.fit(trainx)\n",
    "\n",
    "trainX, trainY = trainx, trainy\n",
    "testX, testY   = testx, testy\n",
    "\n",
    "for i in range(0):\n",
    "    iterate = datagen.flow(trainx, trainy, batch_size=len(trainx), shuffle=True)\n",
    "    x, y = iterate.next()\n",
    "    trainX, trainY = np.append(trainX, x, axis=0), np.append(trainY, y, axis=0)\n",
    "\n",
    "for i in range(0):\n",
    "    iterate = datagen.flow(testx, testy, batch_size=len(testx), shuffle=True)\n",
    "    x, y = iterate.next()\n",
    "    testX, testY = np.append(testX, x, axis=0), np.append(testY, y, axis=0)\n",
    "\n",
    "\n",
    "print(trainX.shape, \"y\", trainY.shape)\n",
    "print(testX.shape, \"y\", testY.shape)\n",
    "\n",
    "trainx, trainy = trainX, trainY\n",
    "testx, testy   = testX, testY\n",
    "\n",
    "trainx = trainx.astype('float32') / 255\n",
    "testx  = testx.astype('float32') / 255\n",
    "\n",
    "trainx_mean = np.mean(trainx, axis=(0, 1, 2))\n",
    "trainx_std  = np.std(trainx, axis=(0, 1, 2))\n",
    "\n",
    "print(trainx_mean, trainx_std)\n",
    "\n",
    "trainx -= trainx_mean\n",
    "trainx /= trainx_std\n",
    "\n",
    "testx -= trainx_mean\n",
    "testx /= trainx_std\n",
    "\n",
    "trainX, trainY = trainx, trainy #utils.to_binary(trainy)\n",
    "testX,  testY  = testx,  testy  #utils.to_binary(testy)\n",
    "\n",
    "min_pix, max_pix = trainX.min(), trainX.max()\n",
    "\n",
    "print(min_pix, max_pix)\n",
    "print(testX.min(), testX.max())\n",
    "\n",
    "print(trainX.shape, trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "K-vi5P3dSZBh",
    "outputId": "78a61916-8818-44c5-8ddf-9c2a7db96b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ik_4kxmzJGoo"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, Activation, MaxPool2D\n",
    "from tensorflow.keras.layers import add, Input, Dense, Flatten, GlobalAvgPool2D, GlobalAvgPool1D\n",
    "from tensorflow.keras.initializers import zeros\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def ResConv(x, kernel=(3, 3), depth=32, maxpool=False):\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    if maxpool :\n",
    "        x = MaxPool2D()(x)\n",
    "    x = Conv2D(depth, kernel, padding='same', use_bias=False)(x)\n",
    "    return x\n",
    "\n",
    "def ResUnit(x, depth=32, maxpool=False):\n",
    "    x = ResConv(x, depth=depth, maxpool=maxpool)\n",
    "    x = ResConv(x, depth=depth)\n",
    "    return x\n",
    "    \n",
    "def ResNetBlock(x, nunit, depth=32, maxpool=False):\n",
    "    assert nunit > 0, \"Ensure there are at least 1 unit in the ResNet Block\"\n",
    "    nunit -= 1\n",
    "    if maxpool:\n",
    "        xskip = Conv2D(depth, (1, 1), strides=2, use_bias=False)(x)\n",
    "    else: \n",
    "        xskip = x\n",
    "    x = add([ResUnit(x, depth=depth, maxpool=maxpool), xskip])\n",
    "    if nunit >= 1:\n",
    "        nunit -= 1\n",
    "        for i in range(nunit):\n",
    "            x = add([ResUnit(x, depth=depth), x])\n",
    "        x = add([ResUnit(x, depth=depth), x])\n",
    "    return x\n",
    "\n",
    "# Returns latent vector of 32 bytes\n",
    "def ResNet9(x):\n",
    "    x = Conv2D(64, (7, 7), strides=2, padding='same', use_bias=False)(x)\n",
    "    x = MaxPool2D((3, 3),  strides=2, padding='same')(x)\n",
    "    \n",
    "    nunits   = (2, 3, 2)\n",
    "    maxpools = (False, True, True)\n",
    "    depths   = (64, 32, 32)\n",
    "    \n",
    "    for i in range(3):\n",
    "        x = ResNetBlock(x, nunits[i], depth=depths[i], maxpool=maxpools[i])\n",
    "    x = GlobalAvgPool2D()(x)\n",
    "    return x\n",
    "\n",
    "#def D_init():\n",
    "#    xin = Input(shape=(64, 64, 3))\n",
    "#    x = ResNet9(xin)\n",
    "#    x = Dense(1, use_bias=False, activation='sigmoid')(x)\n",
    "#    return Model(xin, x)\n",
    "\n",
    "def Q_init():\n",
    "    xin = Input(shape=(64, 64, 3))\n",
    "    x = ResNet9(xin)\n",
    "    x = Dense(1, use_bias=False, activation='sigmoid')(x)\n",
    "    return Model(xin, x)\n",
    "\n",
    "\n",
    "def D_init():\n",
    "    xin = Input(shape=(64, 64, 3))\n",
    "    x = Conv2D(1,(64, 64), use_bias=False, kernel_regularizer=regularizers.l2(0.01))(xin) \n",
    "    x = Flatten()(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    return Model(xin, x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import UpSampling2D, Reshape, Conv2DTranspose\n",
    "\n",
    "def InvResConv(x, kernel=(3, 3), depth=32, upscale=False):\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    if upscale :\n",
    "        x = UpSampling2D((2,2))(x)\n",
    "    x = Conv2D(depth, kernel, padding='same', use_bias=False)(x)\n",
    "    return x\n",
    "\n",
    "def InvResUnit(x, depth=32, upscale=False):\n",
    "    x = InvResConv(x, depth=depth)\n",
    "    x = InvResConv(x, depth=depth, upscale=upscale)\n",
    "    return x\n",
    "    \n",
    "def InvResNetBlock(x, nunit, depth=32, upscale=False):\n",
    "    assert nunit > 0, \"Ensure there are at least 1 unit in the ResNet Block\"\n",
    "    nunit -= 1\n",
    "    x = InvResUnit(x, depth=depth, upscale=upscale)\n",
    "    if nunit >= 1:\n",
    "        nunit -= 1\n",
    "        for i in range(nunit):\n",
    "            x = InvResUnit(x, depth=depth)\n",
    "        x = InvResUnit(x, depth=depth)\n",
    "    return x\n",
    "\n",
    "\n",
    "def InvResNet9(x, size=(4,4)):\n",
    "    depths   = (32, 32, 64)\n",
    "    upscales = (True, True, False)\n",
    "    nunits   = (2, 3, 2)\n",
    "\n",
    "    x = UpSampling2D(size)(x)\n",
    "    for i in range(3):\n",
    "        x = InvResNetBlock(x, nunits[i], depth=depths[i], upscale=upscales[i])\n",
    "    x = UpSampling2D((2,2))(x)\n",
    "    x = Conv2DTranspose(3, (7, 7), strides=2, padding='same', use_bias=False)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_init():\n",
    "    xin = Input(shape=(32,), name=\"Input\")\n",
    "    x = Reshape((1,1,32))(xin)\n",
    "    x = InvResNet9(x)\n",
    "    x = Activation('relu')(x)\n",
    "    model = Model(xin, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPClHV_yKfSx"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        img_h, img_w, img_c = input_img.shape\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = 8 #int(np.sqrt(s / r))\n",
    "            h = 8 #int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "        else:\n",
    "            c = 0.0 #np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w, :] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        fill_mode = 'constant',\n",
    "        cval=0,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=8,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=8,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        preprocessing_function=get_random_eraser(v_l=min_pix, v_h=max_pix, pixel_level=False)\n",
    ")\n",
    "datagen.fit(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzXg6VXY0fbU"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def accuracy(test_x, test_y, model):\n",
    "    result = model.predict(test_x)\n",
    "    predicted_class = np.argmax(result)\n",
    "    true_class = np.argmax(test_y)\n",
    "    num_correct = np.sum(predicted_class == true_class) \n",
    "    accuracy = float(num_correct)/result.shape[0]\n",
    "    return (accuracy * 100)\n",
    "\n",
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['bacc'])+1),model_history.history['bacc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_bacc'])+1),model_history.history['val_bacc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['bacc'])+1),len(model_history.history['bacc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 32, 32, 64)   9408        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 64)   0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 16, 16, 64)   256         max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 16, 16, 64)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 16, 16, 64)   36864       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 64)   256         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 16, 16, 64)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 64)   36864       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 16, 64)   0           conv2d_34[0][0]                  \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 16, 16, 64)   256         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 16, 16, 64)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 16, 16, 64)   36864       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 16, 16, 64)   256         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16, 16, 64)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 16, 16, 64)   36864       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 64)   0           conv2d_36[0][0]                  \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 16, 16, 64)   256         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 16, 16, 64)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 32)     18432       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 8, 8, 32)     128         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 32)     0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 8, 32)     9216        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 8, 32)     2048        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 32)     0           conv2d_39[0][0]                  \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 8, 8, 32)     128         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 32)     0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 8, 8, 32)     9216        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 8, 8, 32)     128         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 8, 32)     0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 8, 8, 32)     9216        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 8, 8, 32)     0           conv2d_41[0][0]                  \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 8, 8, 32)     128         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 32)     0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 8, 8, 32)     9216        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 8, 8, 32)     128         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 8, 8, 32)     0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 8, 8, 32)     9216        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 8, 8, 32)     0           conv2d_43[0][0]                  \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 8, 8, 32)     128         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 8, 8, 32)     0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 4, 4, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 4, 4, 32)     9216        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 4, 4, 32)     128         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 4, 4, 32)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 4, 4, 32)     9216        activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 4, 4, 32)     1024        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 4, 4, 32)     0           conv2d_46[0][0]                  \n",
      "                                                                 conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 4, 4, 32)     128         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 4, 4, 32)     0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 4, 4, 32)     9216        activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 4, 4, 32)     128         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 4, 4, 32)     0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 4, 4, 32)     9216        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 4, 4, 32)     0           conv2d_48[0][0]                  \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 32)           0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            32          global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 263,776\n",
      "Trainable params: 262,560\n",
      "Non-trainable params: 1,216\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "WT_DECAY   = 1e-5\n",
    "MOMENTUM   = 0.90\n",
    "LEARNING_RATE = 0.002\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy, binary_crossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def l2_weights(model):\n",
    "    l2 = 0\n",
    "    for layer in model.layers: \n",
    "        wt = layer.weights\n",
    "        if len(wt) > 0:\n",
    "            l2 += K.sum(K.pow(wt, 2))\n",
    "    return l2\n",
    "\n",
    "def reg_loss(model):\n",
    "    def rloss(y_true, y_pred):\n",
    "        return WT_DECAY*l2_weights(model)\n",
    "    return rloss\n",
    "\n",
    "def loss_with_regularization(model):\n",
    "    def loss(y_true, y_pred):\n",
    "        return binary_crossentropy(y_true, y_pred, True) + reg_loss(model)(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def bacc(y_true, y_pred):\n",
    "    y_pred = K.cast(y_pred > 0.5, dtype=y_true.dtype)\n",
    "    return K.mean(K.equal(y_true, y_pred), axis=-1)\n",
    "\n",
    "Q = Q_init()\n",
    "optimizer = SGD(lr=LEARNING_RATE, momentum=MOMENTUM, nesterov=True)\n",
    "Q.compile(optimizer=optimizer, loss=loss_with_regularization(Q), metrics=[bacc, reg_loss(Q)])\n",
    "Q.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 1, 1, 1)           12288     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 12,288\n",
      "Trainable params: 12,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "D = D_init()\n",
    "\n",
    "D.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['acc'])\n",
    "D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 4, 4, 32)          9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 8, 8, 32)          9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 8, 8, 32)          9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 8, 8, 32)          9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 8, 8, 32)          9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 16, 16, 32)        9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 16, 16, 32)        9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 16, 16, 32)        9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 16, 16, 32)        9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 16, 16, 32)        9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 16, 16, 64)        18432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 16, 16, 64)        36864     \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_62 (Conv2D)           (None, 16, 16, 64)        36864     \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_63 (Conv2D)           (None, 16, 16, 64)        36864     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 64, 64, 3)         9408      \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 64, 64, 3)         0         \n",
      "=================================================================\n",
      "Total params: 232,768\n",
      "Trainable params: 231,680\n",
      "Non-trainable params: 1,088\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "G = G_init()\n",
    "\n",
    "G.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred)*1e6\n",
    "\n",
    "def define_gan(g_model, d_model, q_model):\n",
    "    d_model.trainable = False\n",
    "    q_model.trainable = False\n",
    "    d_output = d_model(g_model.output)\n",
    "    q_output = q_model(g_model.output)\n",
    "    gan = Model(g_model.input, [d_output, q_output])\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    gan.compile(loss=[gan_loss, 'binary_crossentropy'], optimizer=opt, metrics=['acc'])\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 3.3150 - bacc: 0.9748 - rloss: 2.8200 - val_loss: 3.0968 - val_bacc: 0.5224 - val_rloss: 2.5120\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 2.7873 - bacc: 0.9719 - rloss: 2.2901 - val_loss: 2.6060 - val_bacc: 0.5224 - val_rloss: 2.0422\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 2.3570 - bacc: 0.9793 - rloss: 1.8633 - val_loss: 2.2269 - val_bacc: 0.5224 - val_rloss: 1.6636\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 2.0129 - bacc: 0.9748 - rloss: 1.5195 - val_loss: 1.9216 - val_bacc: 0.5224 - val_rloss: 1.3582\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 1.7363 - bacc: 0.9689 - rloss: 1.2419 - val_loss: 1.6751 - val_bacc: 0.5224 - val_rloss: 1.1118\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 1.5079 - bacc: 0.9837 - rloss: 1.0178 - val_loss: 1.6829 - val_bacc: 0.5224 - val_rloss: 0.9126\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 1.3337 - bacc: 0.9733 - rloss: 0.8370 - val_loss: 1.3156 - val_bacc: 0.5224 - val_rloss: 0.7523\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 1.1837 - bacc: 0.9748 - rloss: 0.6911 - val_loss: 1.1859 - val_bacc: 0.5224 - val_rloss: 0.6226\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 1.0677 - bacc: 0.9719 - rloss: 0.5732 - val_loss: 1.0812 - val_bacc: 0.5224 - val_rloss: 0.5179\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.9771 - bacc: 0.9689 - rloss: 0.4778 - val_loss: 0.9963 - val_bacc: 0.5224 - val_rloss: 0.4331\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.8948 - bacc: 0.9778 - rloss: 0.4007 - val_loss: 0.9277 - val_bacc: 0.5224 - val_rloss: 0.3644\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.8314 - bacc: 0.9778 - rloss: 0.3383 - val_loss: 0.8722 - val_bacc: 0.5224 - val_rloss: 0.3089\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.7804 - bacc: 0.9822 - rloss: 0.2878 - val_loss: 0.8273 - val_bacc: 0.5224 - val_rloss: 0.2641\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.7410 - bacc: 0.9778 - rloss: 0.2468 - val_loss: 0.7905 - val_bacc: 0.5224 - val_rloss: 0.2273\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.7028 - bacc: 0.9867 - rloss: 0.2131 - val_loss: 0.9674 - val_bacc: 0.5224 - val_rloss: 0.1972\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.6781 - bacc: 0.9778 - rloss: 0.1856 - val_loss: 0.7357 - val_bacc: 0.5224 - val_rloss: 0.1725\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.6537 - bacc: 0.9852 - rloss: 0.1629 - val_loss: 0.7147 - val_bacc: 0.5224 - val_rloss: 0.1523\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.6465 - bacc: 0.9674 - rloss: 0.1446 - val_loss: 0.6968 - val_bacc: 0.5224 - val_rloss: 0.1361\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.6266 - bacc: 0.9704 - rloss: 0.1297 - val_loss: 0.6809 - val_bacc: 0.5373 - val_rloss: 0.1226\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.6106 - bacc: 0.9793 - rloss: 0.1174 - val_loss: 0.6704 - val_bacc: 0.5373 - val_rloss: 0.1116\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.6024 - bacc: 0.9674 - rloss: 0.1072 - val_loss: 0.6610 - val_bacc: 0.5373 - val_rloss: 0.1023\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.5901 - bacc: 0.9778 - rloss: 0.0987 - val_loss: 0.6502 - val_bacc: 0.5373 - val_rloss: 0.0946\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.5839 - bacc: 0.9748 - rloss: 0.0916 - val_loss: 0.6453 - val_bacc: 0.5373 - val_rloss: 0.0882\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.5756 - bacc: 0.9867 - rloss: 0.0858 - val_loss: 0.6372 - val_bacc: 0.5224 - val_rloss: 0.0832\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.5764 - bacc: 0.9733 - rloss: 0.0813 - val_loss: 0.6350 - val_bacc: 0.5224 - val_rloss: 0.0789\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.5714 - bacc: 0.9778 - rloss: 0.0772 - val_loss: 0.6279 - val_bacc: 0.5373 - val_rloss: 0.0754\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.5697 - bacc: 0.9763 - rloss: 0.0743 - val_loss: 0.6225 - val_bacc: 0.5373 - val_rloss: 0.0730\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.5649 - bacc: 0.9763 - rloss: 0.0719 - val_loss: 0.6158 - val_bacc: 0.5672 - val_rloss: 0.0707\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.5581 - bacc: 0.9881 - rloss: 0.0698 - val_loss: 0.6074 - val_bacc: 0.5672 - val_rloss: 0.0687\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.5604 - bacc: 0.9793 - rloss: 0.0681 - val_loss: 0.6126 - val_bacc: 0.5672 - val_rloss: 0.0673\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.5624 - bacc: 0.9704 - rloss: 0.0667 - val_loss: 0.6131 - val_bacc: 0.5672 - val_rloss: 0.0659\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.5601 - bacc: 0.9778 - rloss: 0.0653 - val_loss: 0.6035 - val_bacc: 0.5821 - val_rloss: 0.0647\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.5586 - bacc: 0.9793 - rloss: 0.0643 - val_loss: 0.6192 - val_bacc: 0.5373 - val_rloss: 0.0638\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.5609 - bacc: 0.9674 - rloss: 0.0634 - val_loss: 0.6082 - val_bacc: 0.5672 - val_rloss: 0.0631\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.5581 - bacc: 0.9689 - rloss: 0.0629 - val_loss: 0.6059 - val_bacc: 0.5522 - val_rloss: 0.0626\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.5572 - bacc: 0.9748 - rloss: 0.0625 - val_loss: 0.5869 - val_bacc: 0.6418 - val_rloss: 0.0623\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.5573 - bacc: 0.9748 - rloss: 0.0621 - val_loss: 0.6021 - val_bacc: 0.5970 - val_rloss: 0.0618\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.5629 - bacc: 0.9511 - rloss: 0.0616 - val_loss: 0.5743 - val_bacc: 0.6716 - val_rloss: 0.0613\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.5606 - bacc: 0.9659 - rloss: 0.0610 - val_loss: 0.5805 - val_bacc: 0.6418 - val_rloss: 0.0608\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.5535 - bacc: 0.9793 - rloss: 0.0607 - val_loss: 0.5751 - val_bacc: 0.6567 - val_rloss: 0.0607\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.5545 - bacc: 0.9748 - rloss: 0.0607 - val_loss: 0.5906 - val_bacc: 0.6119 - val_rloss: 0.0607\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.5505 - bacc: 0.9793 - rloss: 0.0606 - val_loss: 0.5854 - val_bacc: 0.6269 - val_rloss: 0.0605\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.5489 - bacc: 0.9852 - rloss: 0.0605 - val_loss: 0.5911 - val_bacc: 0.6119 - val_rloss: 0.0604\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.5564 - bacc: 0.9778 - rloss: 0.0603 - val_loss: 0.5955 - val_bacc: 0.5821 - val_rloss: 0.0602\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.5525 - bacc: 0.9793 - rloss: 0.0600 - val_loss: 0.5980 - val_bacc: 0.5821 - val_rloss: 0.0599\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.5595 - bacc: 0.9659 - rloss: 0.0598 - val_loss: 0.5814 - val_bacc: 0.6418 - val_rloss: 0.0596\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.5555 - bacc: 0.9733 - rloss: 0.0594 - val_loss: 0.5663 - val_bacc: 0.6866 - val_rloss: 0.0594\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.5529 - bacc: 0.9793 - rloss: 0.0594 - val_loss: 0.7088 - val_bacc: 0.6866 - val_rloss: 0.0594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.5480 - bacc: 0.9852 - rloss: 0.0594 - val_loss: 0.5644 - val_bacc: 0.6716 - val_rloss: 0.0594\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.5534 - bacc: 0.9822 - rloss: 0.0594 - val_loss: 0.5799 - val_bacc: 0.6567 - val_rloss: 0.0593\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.5491 - bacc: 0.9837 - rloss: 0.0592 - val_loss: 0.5776 - val_bacc: 0.6567 - val_rloss: 0.0593\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.5507 - bacc: 0.9822 - rloss: 0.0594 - val_loss: 0.5732 - val_bacc: 0.6567 - val_rloss: 0.0595\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.5533 - bacc: 0.9748 - rloss: 0.0594 - val_loss: 0.5681 - val_bacc: 0.6716 - val_rloss: 0.0594\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.5493 - bacc: 0.9852 - rloss: 0.0593 - val_loss: 0.5509 - val_bacc: 0.7313 - val_rloss: 0.0592\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.5543 - bacc: 0.9689 - rloss: 0.0590 - val_loss: 0.5575 - val_bacc: 0.6716 - val_rloss: 0.0591\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.5513 - bacc: 0.9763 - rloss: 0.0591 - val_loss: 0.5579 - val_bacc: 0.7015 - val_rloss: 0.0592\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.5518 - bacc: 0.9793 - rloss: 0.0594 - val_loss: 0.5682 - val_bacc: 0.7015 - val_rloss: 0.0595\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.5567 - bacc: 0.9719 - rloss: 0.0594 - val_loss: 0.5589 - val_bacc: 0.6716 - val_rloss: 0.0592\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.5509 - bacc: 0.9748 - rloss: 0.0592 - val_loss: 0.5484 - val_bacc: 0.7164 - val_rloss: 0.0592\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.5545 - bacc: 0.9733 - rloss: 0.0593 - val_loss: 0.5437 - val_bacc: 0.7164 - val_rloss: 0.0593\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.5589 - bacc: 0.9719 - rloss: 0.0594 - val_loss: 0.5481 - val_bacc: 0.7313 - val_rloss: 0.0595\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.5514 - bacc: 0.9748 - rloss: 0.0596 - val_loss: 0.5379 - val_bacc: 0.7463 - val_rloss: 0.0598\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.5541 - bacc: 0.9778 - rloss: 0.0597 - val_loss: 0.5469 - val_bacc: 0.7164 - val_rloss: 0.0596\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.5548 - bacc: 0.9763 - rloss: 0.0595 - val_loss: 0.6941 - val_bacc: 0.7313 - val_rloss: 0.0595\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.5500 - bacc: 0.9793 - rloss: 0.0594 - val_loss: 0.5545 - val_bacc: 0.7015 - val_rloss: 0.0593\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.5526 - bacc: 0.9778 - rloss: 0.0593 - val_loss: 0.5541 - val_bacc: 0.7015 - val_rloss: 0.0592\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.5547 - bacc: 0.9733 - rloss: 0.0590 - val_loss: 0.5412 - val_bacc: 0.7015 - val_rloss: 0.0588\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 1s 60ms/step - loss: 0.5494 - bacc: 0.9807 - rloss: 0.0588 - val_loss: 0.5367 - val_bacc: 0.7612 - val_rloss: 0.0590\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.5519 - bacc: 0.9763 - rloss: 0.0591 - val_loss: 0.5430 - val_bacc: 0.7313 - val_rloss: 0.0590\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.5504 - bacc: 0.9837 - rloss: 0.0590 - val_loss: 0.7291 - val_bacc: 0.6567 - val_rloss: 0.0590\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.5538 - bacc: 0.9733 - rloss: 0.0591 - val_loss: 0.5928 - val_bacc: 0.6716 - val_rloss: 0.0592\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.5541 - bacc: 0.9719 - rloss: 0.0593 - val_loss: 0.5408 - val_bacc: 0.7313 - val_rloss: 0.0593\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.5501 - bacc: 0.9837 - rloss: 0.0592 - val_loss: 0.5357 - val_bacc: 0.7463 - val_rloss: 0.0590\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.5506 - bacc: 0.9822 - rloss: 0.0590 - val_loss: 0.5387 - val_bacc: 0.7313 - val_rloss: 0.0590\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.5481 - bacc: 0.9852 - rloss: 0.0590 - val_loss: 0.5499 - val_bacc: 0.7313 - val_rloss: 0.0590\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.5500 - bacc: 0.9852 - rloss: 0.0591 - val_loss: 0.6082 - val_bacc: 0.6567 - val_rloss: 0.0591\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.5501 - bacc: 0.9852 - rloss: 0.0592 - val_loss: 0.5419 - val_bacc: 0.7463 - val_rloss: 0.0593\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.5537 - bacc: 0.9704 - rloss: 0.0594 - val_loss: 0.5326 - val_bacc: 0.7761 - val_rloss: 0.0595\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.5472 - bacc: 0.9837 - rloss: 0.0595 - val_loss: 0.5393 - val_bacc: 0.7612 - val_rloss: 0.0596\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.5484 - bacc: 0.9867 - rloss: 0.0595 - val_loss: 0.5486 - val_bacc: 0.7313 - val_rloss: 0.0594\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.5522 - bacc: 0.9733 - rloss: 0.0593 - val_loss: 0.5621 - val_bacc: 0.7164 - val_rloss: 0.0593\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.5538 - bacc: 0.9733 - rloss: 0.0593 - val_loss: 0.6171 - val_bacc: 0.6716 - val_rloss: 0.0594\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.5511 - bacc: 0.9852 - rloss: 0.0595 - val_loss: 0.5872 - val_bacc: 0.7313 - val_rloss: 0.0595\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.5535 - bacc: 0.9733 - rloss: 0.0595 - val_loss: 0.6100 - val_bacc: 0.6418 - val_rloss: 0.0594\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.5597 - bacc: 0.9689 - rloss: 0.0594 - val_loss: 0.5441 - val_bacc: 0.7313 - val_rloss: 0.0593\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.5533 - bacc: 0.9763 - rloss: 0.0593 - val_loss: 0.5890 - val_bacc: 0.6866 - val_rloss: 0.0594\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.5527 - bacc: 0.9793 - rloss: 0.0595 - val_loss: 0.5360 - val_bacc: 0.7463 - val_rloss: 0.0597\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.5554 - bacc: 0.9733 - rloss: 0.0597 - val_loss: 0.5388 - val_bacc: 0.7463 - val_rloss: 0.0597\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.5534 - bacc: 0.9763 - rloss: 0.0598 - val_loss: 0.5426 - val_bacc: 0.7164 - val_rloss: 0.0597\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.5523 - bacc: 0.9778 - rloss: 0.0599 - val_loss: 0.5441 - val_bacc: 0.7015 - val_rloss: 0.0600\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.5511 - bacc: 0.9778 - rloss: 0.0601 - val_loss: 0.5383 - val_bacc: 0.7761 - val_rloss: 0.0602\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.5529 - bacc: 0.9793 - rloss: 0.0602 - val_loss: 0.5444 - val_bacc: 0.7463 - val_rloss: 0.0602\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.5491 - bacc: 0.9896 - rloss: 0.0603 - val_loss: 0.5482 - val_bacc: 0.7015 - val_rloss: 0.0604\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.5522 - bacc: 0.9807 - rloss: 0.0603 - val_loss: 0.5509 - val_bacc: 0.6866 - val_rloss: 0.0603\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 1s 60ms/step - loss: 0.5512 - bacc: 0.9837 - rloss: 0.0602 - val_loss: 0.5436 - val_bacc: 0.7164 - val_rloss: 0.0602\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.5557 - bacc: 0.9733 - rloss: 0.0603 - val_loss: 0.5466 - val_bacc: 0.6567 - val_rloss: 0.0602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.5527 - bacc: 0.9763 - rloss: 0.0602 - val_loss: 0.5348 - val_bacc: 0.7910 - val_rloss: 0.0602\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.5516 - bacc: 0.9852 - rloss: 0.0602 - val_loss: 0.5513 - val_bacc: 0.6866 - val_rloss: 0.0602\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.5514 - bacc: 0.9793 - rloss: 0.0602 - val_loss: 0.5666 - val_bacc: 0.6716 - val_rloss: 0.0604\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.5504 - bacc: 0.9852 - rloss: 0.0605 - val_loss: 0.5630 - val_bacc: 0.7313 - val_rloss: 0.0606\n",
      "Train on 675 samples, validate on 67 samples\n",
      "Epoch 1/200\n",
      "675/675 [==============================] - 5s 7ms/sample - loss: 0.0855 - model_7_loss: 1.8091e-05 - model_6_loss: 0.0680 - model_7_acc: 1.0000 - model_6_acc: 0.9778 - val_loss: 2.1295 - val_model_7_loss: 3.5025e-11 - val_model_6_loss: 2.5506 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6269\n",
      "Epoch 2/200\n",
      "675/675 [==============================] - 1s 963us/sample - loss: 0.0904 - model_7_loss: 8.3661e-07 - model_6_loss: 0.0733 - model_7_acc: 1.0000 - model_6_acc: 0.9778 - val_loss: 1.3086 - val_model_7_loss: 3.2652e-12 - val_model_6_loss: 2.1276 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 3/200\n",
      "675/675 [==============================] - 1s 979us/sample - loss: 0.1033 - model_7_loss: 4.4757e-06 - model_6_loss: 0.1604 - model_7_acc: 1.0000 - model_6_acc: 0.9674 - val_loss: 1.1808 - val_model_7_loss: 1.9762e-12 - val_model_6_loss: 1.9189 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 4/200\n",
      "675/675 [==============================] - 1s 958us/sample - loss: 0.0755 - model_7_loss: 1.5110e-05 - model_6_loss: 0.0587 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.1309 - val_model_7_loss: 1.4577e-09 - val_model_6_loss: 0.7818 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7463\n",
      "Epoch 5/200\n",
      "675/675 [==============================] - 1s 977us/sample - loss: 0.0649 - model_7_loss: 2.2607e-05 - model_6_loss: 0.0484 - model_7_acc: 1.0000 - model_6_acc: 0.9822 - val_loss: 1.2226 - val_model_7_loss: 1.6468e-07 - val_model_6_loss: 0.8550 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 6/200\n",
      "675/675 [==============================] - 1s 960us/sample - loss: 0.0553 - model_7_loss: 9.5542e-06 - model_6_loss: 0.0391 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.2945 - val_model_7_loss: 3.9260e-06 - val_model_6_loss: 1.6188 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 7/200\n",
      "675/675 [==============================] - 1s 992us/sample - loss: 0.0686 - model_7_loss: 4.8479e-05 - model_6_loss: 0.0517 - model_7_acc: 1.0000 - model_6_acc: 0.9852 - val_loss: 1.0448 - val_model_7_loss: 1.0796e-06 - val_model_6_loss: 0.8284 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 8/200\n",
      "675/675 [==============================] - 1s 955us/sample - loss: 0.0488 - model_7_loss: 1.3814e-05 - model_6_loss: 0.0565 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.7067 - val_model_7_loss: 2.0095e-12 - val_model_6_loss: 1.1910 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6716\n",
      "Epoch 9/200\n",
      "675/675 [==============================] - 1s 961us/sample - loss: 0.0755 - model_7_loss: 4.0786e-09 - model_6_loss: 0.0605 - model_7_acc: 1.0000 - model_6_acc: 0.9852 - val_loss: 1.2938 - val_model_7_loss: 3.2577e-11 - val_model_6_loss: 1.6182 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 10/200\n",
      "675/675 [==============================] - 1s 964us/sample - loss: 0.0686 - model_7_loss: 2.4059e-08 - model_6_loss: 0.1492 - model_7_acc: 1.0000 - model_6_acc: 0.9837 - val_loss: 1.0606 - val_model_7_loss: 9.5914e-13 - val_model_6_loss: 1.0504 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 11/200\n",
      "675/675 [==============================] - 1s 966us/sample - loss: 0.0799 - model_7_loss: 2.4750e-09 - model_6_loss: 0.0627 - model_7_acc: 1.0000 - model_6_acc: 0.9807 - val_loss: 1.1540 - val_model_7_loss: 1.2622e-12 - val_model_6_loss: 1.2031 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 12/200\n",
      "675/675 [==============================] - 1s 992us/sample - loss: 0.0452 - model_7_loss: 6.1808e-08 - model_6_loss: 0.0299 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.2575 - val_model_7_loss: 5.4152e-10 - val_model_6_loss: 0.8683 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 13/200\n",
      "675/675 [==============================] - 1s 952us/sample - loss: 0.0671 - model_7_loss: 2.3725e-08 - model_6_loss: 0.1923 - model_7_acc: 1.0000 - model_6_acc: 0.9852 - val_loss: 1.1556 - val_model_7_loss: 4.2803e-13 - val_model_6_loss: 0.8932 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6269\n",
      "Epoch 14/200\n",
      "675/675 [==============================] - 1s 987us/sample - loss: 0.1033 - model_7_loss: 1.5628e-08 - model_6_loss: 0.0853 - model_7_acc: 1.0000 - model_6_acc: 0.9748 - val_loss: 1.0776 - val_model_7_loss: 1.9317e-12 - val_model_6_loss: 1.7280 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 15/200\n",
      "675/675 [==============================] - 1s 978us/sample - loss: 0.0562 - model_7_loss: 6.8662e-07 - model_6_loss: 0.0402 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.1508 - val_model_7_loss: 1.6997e-10 - val_model_6_loss: 1.5045 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7463\n",
      "Epoch 16/200\n",
      "675/675 [==============================] - 1s 978us/sample - loss: 0.0797 - model_7_loss: 1.4117e-06 - model_6_loss: 0.1789 - model_7_acc: 1.0000 - model_6_acc: 0.9822 - val_loss: 0.9477 - val_model_7_loss: 4.9538e-08 - val_model_6_loss: 0.6913 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 17/200\n",
      "675/675 [==============================] - 1s 979us/sample - loss: 0.0941 - model_7_loss: 2.6276e-04 - model_6_loss: 0.1501 - model_7_acc: 1.0000 - model_6_acc: 0.9763 - val_loss: 0.8647 - val_model_7_loss: 4.3994e-08 - val_model_6_loss: 0.6037 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7612\n",
      "Epoch 18/200\n",
      "675/675 [==============================] - 1s 981us/sample - loss: 0.0922 - model_7_loss: 2.4662e-04 - model_6_loss: 0.0744 - model_7_acc: 1.0000 - model_6_acc: 0.9807 - val_loss: 0.8935 - val_model_7_loss: 5.4785e-06 - val_model_6_loss: 1.2276 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 19/200\n",
      "675/675 [==============================] - 1s 995us/sample - loss: 0.0752 - model_7_loss: 1.0150e-04 - model_6_loss: 0.1542 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.0072 - val_model_7_loss: 1.0071e-06 - val_model_6_loss: 0.6968 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 20/200\n",
      "675/675 [==============================] - 1s 964us/sample - loss: 0.0797 - model_7_loss: 1.3233e-04 - model_6_loss: 0.0828 - model_7_acc: 1.0000 - model_6_acc: 0.9807 - val_loss: 1.0406 - val_model_7_loss: 4.4081e-05 - val_model_6_loss: 0.7191 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 21/200\n",
      "675/675 [==============================] - 1s 969us/sample - loss: 0.0815 - model_7_loss: 6.9244e-04 - model_6_loss: 0.0645 - model_7_acc: 1.0000 - model_6_acc: 0.9807 - val_loss: 1.0646 - val_model_7_loss: 1.8122e-06 - val_model_6_loss: 1.4279 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 22/200\n",
      "675/675 [==============================] - 1s 965us/sample - loss: 0.0633 - model_7_loss: 1.5970e-06 - model_6_loss: 0.1176 - model_7_acc: 1.0000 - model_6_acc: 0.9837 - val_loss: 1.2840 - val_model_7_loss: 1.3756e-08 - val_model_6_loss: 0.8988 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6418\n",
      "Epoch 23/200\n",
      "675/675 [==============================] - 1s 1ms/sample - loss: 0.0805 - model_7_loss: 3.0652e-05 - model_6_loss: 0.0640 - model_7_acc: 1.0000 - model_6_acc: 0.9822 - val_loss: 1.2598 - val_model_7_loss: 1.6961e-05 - val_model_6_loss: 1.1219 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 24/200\n",
      "675/675 [==============================] - 1s 969us/sample - loss: 0.0540 - model_7_loss: 5.3241e-06 - model_6_loss: 0.0378 - model_7_acc: 1.0000 - model_6_acc: 0.9852 - val_loss: 1.2184 - val_model_7_loss: 2.9526e-06 - val_model_6_loss: 0.8478 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6716\n",
      "Epoch 25/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 950us/sample - loss: 0.0609 - model_7_loss: 8.9581e-06 - model_6_loss: 0.1095 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.0530 - val_model_7_loss: 3.8688e-07 - val_model_6_loss: 0.7465 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 26/200\n",
      "675/675 [==============================] - 1s 971us/sample - loss: 0.0614 - model_7_loss: 1.4038e-04 - model_6_loss: 0.0452 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.1376 - val_model_7_loss: 1.9245e-09 - val_model_6_loss: 0.7878 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6716\n",
      "Epoch 27/200\n",
      "675/675 [==============================] - 1s 989us/sample - loss: 0.0425 - model_7_loss: 5.4677e-05 - model_6_loss: 0.0266 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.3462 - val_model_7_loss: 1.7990e-07 - val_model_6_loss: 1.9559 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 28/200\n",
      "675/675 [==============================] - 1s 1ms/sample - loss: 0.0637 - model_7_loss: 1.0927e-05 - model_6_loss: 0.1426 - model_7_acc: 1.0000 - model_6_acc: 0.9837 - val_loss: 0.9785 - val_model_7_loss: 1.2760e-06 - val_model_6_loss: 0.6855 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 29/200\n",
      "675/675 [==============================] - 1s 955us/sample - loss: 0.0629 - model_7_loss: 6.8778e-05 - model_6_loss: 0.0469 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 0.8945 - val_model_7_loss: 9.3818e-07 - val_model_6_loss: 0.6321 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 30/200\n",
      "675/675 [==============================] - 1s 971us/sample - loss: 0.0477 - model_7_loss: 1.4155e-05 - model_6_loss: 0.0318 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.0090 - val_model_7_loss: 7.0801e-07 - val_model_6_loss: 0.6974 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7463\n",
      "Epoch 31/200\n",
      "675/675 [==============================] - 1s 969us/sample - loss: 0.0565 - model_7_loss: 3.1208e-06 - model_6_loss: 0.0408 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.1989 - val_model_7_loss: 4.7417e-07 - val_model_6_loss: 0.8752 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 32/200\n",
      "675/675 [==============================] - 1s 965us/sample - loss: 0.0522 - model_7_loss: 3.3199e-06 - model_6_loss: 0.1284 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.3871 - val_model_7_loss: 5.6709e-09 - val_model_6_loss: 1.7746 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6418\n",
      "Epoch 33/200\n",
      "675/675 [==============================] - 1s 969us/sample - loss: 0.0558 - model_7_loss: 4.4471e-08 - model_6_loss: 0.0395 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.4259 - val_model_7_loss: 1.1746e-09 - val_model_6_loss: 1.7769 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 34/200\n",
      "675/675 [==============================] - 1s 954us/sample - loss: 0.0585 - model_7_loss: 2.9100e-07 - model_6_loss: 0.0428 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.2893 - val_model_7_loss: 2.1496e-09 - val_model_6_loss: 1.1078 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 35/200\n",
      "675/675 [==============================] - 1s 958us/sample - loss: 0.0550 - model_7_loss: 2.2111e-05 - model_6_loss: 0.0680 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.3824 - val_model_7_loss: 2.8113e-09 - val_model_6_loss: 1.4615 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 36/200\n",
      "675/675 [==============================] - 1s 970us/sample - loss: 0.0664 - model_7_loss: 2.7249e-06 - model_6_loss: 0.1637 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.0157 - val_model_7_loss: 3.2031e-07 - val_model_6_loss: 0.7030 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 37/200\n",
      "675/675 [==============================] - 1s 950us/sample - loss: 0.0699 - model_7_loss: 4.6779e-04 - model_6_loss: 0.0526 - model_7_acc: 1.0000 - model_6_acc: 0.9822 - val_loss: 1.2051 - val_model_7_loss: 2.4551e-07 - val_model_6_loss: 1.6327 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 38/200\n",
      "675/675 [==============================] - 1s 961us/sample - loss: 0.0632 - model_7_loss: 8.3845e-05 - model_6_loss: 0.0470 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.2559 - val_model_7_loss: 2.4032e-06 - val_model_6_loss: 0.8682 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 39/200\n",
      "675/675 [==============================] - 1s 988us/sample - loss: 0.0476 - model_7_loss: 2.2737e-05 - model_6_loss: 0.0316 - model_7_acc: 1.0000 - model_6_acc: 0.9852 - val_loss: 1.3182 - val_model_7_loss: 1.2830e-06 - val_model_6_loss: 0.9145 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 40/200\n",
      "675/675 [==============================] - 1s 982us/sample - loss: 0.0517 - model_7_loss: 6.3987e-05 - model_6_loss: 0.0360 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.4109 - val_model_7_loss: 1.5620e-05 - val_model_6_loss: 0.9951 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 41/200\n",
      "675/675 [==============================] - 1s 979us/sample - loss: 0.0452 - model_7_loss: 7.6263e-05 - model_6_loss: 0.0297 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.3531 - val_model_7_loss: 6.8426e-08 - val_model_6_loss: 1.5698 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 42/200\n",
      "675/675 [==============================] - 1s 946us/sample - loss: 0.0659 - model_7_loss: 1.0633e-05 - model_6_loss: 0.0667 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.0659 - val_model_7_loss: 3.5615e-07 - val_model_6_loss: 2.3384 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7463\n",
      "Epoch 43/200\n",
      "675/675 [==============================] - 1s 977us/sample - loss: 0.0624 - model_7_loss: 8.0096e-05 - model_6_loss: 0.0458 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.0475 - val_model_7_loss: 2.6672e-08 - val_model_6_loss: 1.2400 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 44/200\n",
      "675/675 [==============================] - 1s 962us/sample - loss: 0.0485 - model_7_loss: 4.3772e-05 - model_6_loss: 0.1151 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.3812 - val_model_7_loss: 3.6199e-09 - val_model_6_loss: 1.1236 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6567\n",
      "Epoch 45/200\n",
      "675/675 [==============================] - 1s 969us/sample - loss: 0.0643 - model_7_loss: 2.2906e-04 - model_6_loss: 0.0570 - model_7_acc: 1.0000 - model_6_acc: 0.9807 - val_loss: 1.3277 - val_model_7_loss: 9.3700e-05 - val_model_6_loss: 2.4403 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 46/200\n",
      "675/675 [==============================] - 1s 957us/sample - loss: 0.0545 - model_7_loss: 1.6772e-04 - model_6_loss: 0.1110 - model_7_acc: 1.0000 - model_6_acc: 0.9852 - val_loss: 1.5228 - val_model_7_loss: 2.3293e-06 - val_model_6_loss: 1.8677 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6567\n",
      "Epoch 47/200\n",
      "675/675 [==============================] - 1s 970us/sample - loss: 0.0846 - model_7_loss: 8.2701e-05 - model_6_loss: 0.1350 - model_7_acc: 1.0000 - model_6_acc: 0.9822 - val_loss: 1.2789 - val_model_7_loss: 7.0712e-06 - val_model_6_loss: 2.1567 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 48/200\n",
      "675/675 [==============================] - 1s 942us/sample - loss: 0.0855 - model_7_loss: 2.0360e-06 - model_6_loss: 0.1470 - model_7_acc: 1.0000 - model_6_acc: 0.9778 - val_loss: 1.0659 - val_model_7_loss: 1.8634e-05 - val_model_6_loss: 0.7415 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 49/200\n",
      "675/675 [==============================] - 1s 975us/sample - loss: 0.0664 - model_7_loss: 9.1414e-06 - model_6_loss: 0.0499 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.2239 - val_model_7_loss: 6.3702e-06 - val_model_6_loss: 0.8777 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 50/200\n",
      "675/675 [==============================] - 1s 951us/sample - loss: 0.0648 - model_7_loss: 1.3985e-05 - model_6_loss: 0.0483 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.2201 - val_model_7_loss: 1.5954e-05 - val_model_6_loss: 0.8904 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 51/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 959us/sample - loss: 0.0505 - model_7_loss: 3.6241e-05 - model_6_loss: 0.0348 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.3429 - val_model_7_loss: 8.1486e-06 - val_model_6_loss: 1.3155 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 52/200\n",
      "675/675 [==============================] - 1s 967us/sample - loss: 0.0395 - model_7_loss: 4.4300e-05 - model_6_loss: 0.0239 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.3682 - val_model_7_loss: 1.2003e-05 - val_model_6_loss: 0.9716 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 53/200\n",
      "675/675 [==============================] - 1s 959us/sample - loss: 0.0397 - model_7_loss: 1.0943e-04 - model_6_loss: 0.0238 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.3650 - val_model_7_loss: 1.4750e-06 - val_model_6_loss: 1.0683 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 54/200\n",
      "675/675 [==============================] - 1s 970us/sample - loss: 0.0390 - model_7_loss: 1.9864e-05 - model_6_loss: 0.0233 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.4144 - val_model_7_loss: 1.5735e-05 - val_model_6_loss: 1.1281 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 55/200\n",
      "675/675 [==============================] - 1s 954us/sample - loss: 0.0464 - model_7_loss: 4.5666e-05 - model_6_loss: 0.0308 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.4706 - val_model_7_loss: 3.0049e-06 - val_model_6_loss: 1.0167 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 56/200\n",
      "675/675 [==============================] - 1s 963us/sample - loss: 0.0450 - model_7_loss: 8.7199e-06 - model_6_loss: 0.0632 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.5558 - val_model_7_loss: 2.8879e-07 - val_model_6_loss: 1.6309 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 57/200\n",
      "675/675 [==============================] - 1s 976us/sample - loss: 0.0572 - model_7_loss: 1.7139e-04 - model_6_loss: 0.0406 - model_7_acc: 1.0000 - model_6_acc: 0.9852 - val_loss: 1.4261 - val_model_7_loss: 4.0679e-06 - val_model_6_loss: 0.9937 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 58/200\n",
      "675/675 [==============================] - 1s 962us/sample - loss: 0.0423 - model_7_loss: 7.3966e-05 - model_6_loss: 0.0271 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.3414 - val_model_7_loss: 2.3447e-05 - val_model_6_loss: 1.5600 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 59/200\n",
      "675/675 [==============================] - 1s 976us/sample - loss: 0.0427 - model_7_loss: 3.3524e-05 - model_6_loss: 0.0268 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.5344 - val_model_7_loss: 3.1731e-05 - val_model_6_loss: 1.3871 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 60/200\n",
      "675/675 [==============================] - 1s 953us/sample - loss: 0.0487 - model_7_loss: 8.2214e-07 - model_6_loss: 0.0434 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.5179 - val_model_7_loss: 2.7061e-04 - val_model_6_loss: 1.0908 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 61/200\n",
      "675/675 [==============================] - 1s 998us/sample - loss: 0.0378 - model_7_loss: 3.2009e-06 - model_6_loss: 0.0234 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.6063 - val_model_7_loss: 1.5647e-05 - val_model_6_loss: 1.1137 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6716\n",
      "Epoch 62/200\n",
      "675/675 [==============================] - 1s 959us/sample - loss: 0.0393 - model_7_loss: 2.0290e-05 - model_6_loss: 0.0239 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.6607 - val_model_7_loss: 1.5256e-05 - val_model_6_loss: 1.1489 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 63/200\n",
      "675/675 [==============================] - 1s 971us/sample - loss: 0.0546 - model_7_loss: 1.6212e-05 - model_6_loss: 0.1058 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.3611 - val_model_7_loss: 1.8050e-05 - val_model_6_loss: 1.6886 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6567\n",
      "Epoch 64/200\n",
      "675/675 [==============================] - 1s 951us/sample - loss: 0.0581 - model_7_loss: 6.7028e-07 - model_6_loss: 0.0438 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.5228 - val_model_7_loss: 3.5388e-07 - val_model_6_loss: 1.0999 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 65/200\n",
      "675/675 [==============================] - 1s 953us/sample - loss: 0.0410 - model_7_loss: 9.0729e-07 - model_6_loss: 0.0292 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.4576 - val_model_7_loss: 2.5292e-07 - val_model_6_loss: 1.0194 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 66/200\n",
      "675/675 [==============================] - 1s 963us/sample - loss: 0.0533 - model_7_loss: 7.2185e-06 - model_6_loss: 0.0913 - model_7_acc: 1.0000 - model_6_acc: 0.9852 - val_loss: 1.6961 - val_model_7_loss: 3.1516e-09 - val_model_6_loss: 2.0517 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6269\n",
      "Epoch 67/200\n",
      "675/675 [==============================] - 1s 976us/sample - loss: 0.0859 - model_7_loss: 6.0470e-05 - model_6_loss: 0.0867 - model_7_acc: 1.0000 - model_6_acc: 0.9719 - val_loss: 1.4531 - val_model_7_loss: 2.1775e-08 - val_model_6_loss: 1.6663 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6716\n",
      "Epoch 68/200\n",
      "675/675 [==============================] - 1s 951us/sample - loss: 0.0482 - model_7_loss: 2.5876e-05 - model_6_loss: 0.0868 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.6558 - val_model_7_loss: 2.4618e-06 - val_model_6_loss: 1.6886 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 69/200\n",
      "675/675 [==============================] - 1s 951us/sample - loss: 0.0553 - model_7_loss: 2.5829e-04 - model_6_loss: 0.0393 - model_7_acc: 1.0000 - model_6_acc: 0.9837 - val_loss: 1.5522 - val_model_7_loss: 9.8690e-06 - val_model_6_loss: 1.7364 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 70/200\n",
      "675/675 [==============================] - 1s 957us/sample - loss: 0.0502 - model_7_loss: 1.0724e-04 - model_6_loss: 0.0340 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.4843 - val_model_7_loss: 1.1178e-06 - val_model_6_loss: 1.0554 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 71/200\n",
      "675/675 [==============================] - 1s 964us/sample - loss: 0.0381 - model_7_loss: 2.0623e-06 - model_6_loss: 0.0226 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.4585 - val_model_7_loss: 6.3496e-06 - val_model_6_loss: 1.0079 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 72/200\n",
      "675/675 [==============================] - 1s 983us/sample - loss: 0.0447 - model_7_loss: 1.7131e-05 - model_6_loss: 0.0719 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.6034 - val_model_7_loss: 1.4508e-08 - val_model_6_loss: 1.5430 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 73/200\n",
      "675/675 [==============================] - 1s 965us/sample - loss: 0.0521 - model_7_loss: 7.3553e-07 - model_6_loss: 0.0370 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.4740 - val_model_7_loss: 3.1225e-08 - val_model_6_loss: 1.3720 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 74/200\n",
      "675/675 [==============================] - 1s 971us/sample - loss: 0.0478 - model_7_loss: 6.5559e-06 - model_6_loss: 0.0336 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.3976 - val_model_7_loss: 1.0510e-06 - val_model_6_loss: 0.9656 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 75/200\n",
      "675/675 [==============================] - 1s 986us/sample - loss: 0.0363 - model_7_loss: 5.1115e-05 - model_6_loss: 0.0216 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.5025 - val_model_7_loss: 1.3498e-06 - val_model_6_loss: 1.0384 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 76/200\n",
      "675/675 [==============================] - 1s 969us/sample - loss: 0.0396 - model_7_loss: 2.3530e-05 - model_6_loss: 0.0238 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.5231 - val_model_7_loss: 2.0016e-06 - val_model_6_loss: 1.9816 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 963us/sample - loss: 0.0333 - model_7_loss: 2.4591e-05 - model_6_loss: 0.0179 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.5189 - val_model_7_loss: 9.1758e-07 - val_model_6_loss: 2.2109 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 78/200\n",
      "675/675 [==============================] - 1s 975us/sample - loss: 0.0392 - model_7_loss: 4.8672e-05 - model_6_loss: 0.0411 - model_7_acc: 1.0000 - model_6_acc: 0.9852 - val_loss: 1.6416 - val_model_7_loss: 1.8747e-06 - val_model_6_loss: 1.8169 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 79/200\n",
      "675/675 [==============================] - 1s 964us/sample - loss: 0.0465 - model_7_loss: 2.2244e-06 - model_6_loss: 0.0316 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.5086 - val_model_7_loss: 2.8238e-05 - val_model_6_loss: 1.6510 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 80/200\n",
      "675/675 [==============================] - 1s 981us/sample - loss: 0.0287 - model_7_loss: 5.1697e-06 - model_6_loss: 0.0139 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.6432 - val_model_7_loss: 1.9607e-05 - val_model_6_loss: 2.3028 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 81/200\n",
      "675/675 [==============================] - 1s 972us/sample - loss: 0.0393 - model_7_loss: 4.2508e-06 - model_6_loss: 0.0236 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.6988 - val_model_7_loss: 1.9657e-05 - val_model_6_loss: 1.1883 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 82/200\n",
      "675/675 [==============================] - 1s 953us/sample - loss: 0.0310 - model_7_loss: 7.0605e-06 - model_6_loss: 0.0161 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.7367 - val_model_7_loss: 6.2718e-06 - val_model_6_loss: 2.3328 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 83/200\n",
      "675/675 [==============================] - 1s 937us/sample - loss: 0.0399 - model_7_loss: 1.9285e-05 - model_6_loss: 0.0807 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.9439 - val_model_7_loss: 0.0031 - val_model_6_loss: 1.7596 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 84/200\n",
      "675/675 [==============================] - 1s 960us/sample - loss: 0.2967 - model_7_loss: 0.1641 - model_6_loss: 0.1097 - model_7_acc: 1.0000 - model_6_acc: 0.9719 - val_loss: 1.0764 - val_model_7_loss: 5.8965e-04 - val_model_6_loss: 1.2837 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6418\n",
      "Epoch 85/200\n",
      "675/675 [==============================] - 1s 925us/sample - loss: 0.0913 - model_7_loss: 7.4285e-29 - model_6_loss: 0.0758 - model_7_acc: 1.0000 - model_6_acc: 0.9822 - val_loss: 1.3660 - val_model_7_loss: 2.6551e-04 - val_model_6_loss: 1.0087 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6418\n",
      "Epoch 86/200\n",
      "675/675 [==============================] - 1s 957us/sample - loss: 0.0796 - model_7_loss: 1.1921e-30 - model_6_loss: 0.0648 - model_7_acc: 1.0000 - model_6_acc: 0.9778 - val_loss: 1.3942 - val_model_7_loss: 2.4486e-05 - val_model_6_loss: 1.5096 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6567\n",
      "Epoch 87/200\n",
      "675/675 [==============================] - 1s 963us/sample - loss: 0.0593 - model_7_loss: 8.6532e-30 - model_6_loss: 0.0429 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.3933 - val_model_7_loss: 2.1085e-08 - val_model_6_loss: 1.9009 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6716\n",
      "Epoch 88/200\n",
      "675/675 [==============================] - 1s 969us/sample - loss: 0.0712 - model_7_loss: 2.3919e-30 - model_6_loss: 0.0544 - model_7_acc: 1.0000 - model_6_acc: 0.9822 - val_loss: 1.2791 - val_model_7_loss: 5.3943e-11 - val_model_6_loss: 1.2446 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6716\n",
      "Epoch 89/200\n",
      "675/675 [==============================] - 1s 988us/sample - loss: 0.0608 - model_7_loss: 5.1546e-30 - model_6_loss: 0.0458 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.4962 - val_model_7_loss: 3.2708e-13 - val_model_6_loss: 1.5233 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6567\n",
      "Epoch 90/200\n",
      "675/675 [==============================] - 1s 955us/sample - loss: 0.0540 - model_7_loss: 4.3084e-30 - model_6_loss: 0.0378 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.4119 - val_model_7_loss: 4.6842e-16 - val_model_6_loss: 1.0011 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6567\n",
      "Epoch 91/200\n",
      "675/675 [==============================] - 1s 1ms/sample - loss: 0.0770 - model_7_loss: 8.2222e-30 - model_6_loss: 0.0717 - model_7_acc: 1.0000 - model_6_acc: 0.9822 - val_loss: 1.4864 - val_model_7_loss: 9.2966e-17 - val_model_6_loss: 1.0276 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6716\n",
      "Epoch 92/200\n",
      "675/675 [==============================] - 1s 986us/sample - loss: 0.0478 - model_7_loss: 3.1542e-29 - model_6_loss: 0.1047 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.2398 - val_model_7_loss: 4.0176e-20 - val_model_6_loss: 0.8608 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 93/200\n",
      "675/675 [==============================] - 1s 983us/sample - loss: 0.0865 - model_7_loss: 9.5920e-29 - model_6_loss: 0.1378 - model_7_acc: 1.0000 - model_6_acc: 0.9793 - val_loss: 1.2376 - val_model_7_loss: 1.9924e-22 - val_model_6_loss: 1.4327 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6716\n",
      "Epoch 94/200\n",
      "675/675 [==============================] - 1s 1ms/sample - loss: 0.0805 - model_7_loss: 2.2811e-29 - model_6_loss: 0.0674 - model_7_acc: 1.0000 - model_6_acc: 0.9793 - val_loss: 1.2067 - val_model_7_loss: 6.4791e-24 - val_model_6_loss: 0.8572 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 95/200\n",
      "675/675 [==============================] - 1s 963us/sample - loss: 0.0704 - model_7_loss: 2.0553e-28 - model_6_loss: 0.1394 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.1094 - val_model_7_loss: 3.2048e-25 - val_model_6_loss: 1.3415 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 96/200\n",
      "675/675 [==============================] - 1s 938us/sample - loss: 0.0542 - model_7_loss: 1.1484e-29 - model_6_loss: 0.0571 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.2654 - val_model_7_loss: 5.3025e-24 - val_model_6_loss: 1.1993 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6567\n",
      "Epoch 97/200\n",
      "675/675 [==============================] - 1s 923us/sample - loss: 0.0706 - model_7_loss: 1.7451e-29 - model_6_loss: 0.2213 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 0.9754 - val_model_7_loss: 2.0302e-28 - val_model_6_loss: 0.7255 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 98/200\n",
      "675/675 [==============================] - 1s 958us/sample - loss: 0.0649 - model_7_loss: 4.1603e-29 - model_6_loss: 0.0488 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.0820 - val_model_7_loss: 6.4225e-29 - val_model_6_loss: 1.5892 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 99/200\n",
      "675/675 [==============================] - 1s 959us/sample - loss: 0.0455 - model_7_loss: 1.1467e-27 - model_6_loss: 0.0301 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.1917 - val_model_7_loss: 4.8003e-28 - val_model_6_loss: 0.8573 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 100/200\n",
      "675/675 [==============================] - 1s 958us/sample - loss: 0.0528 - model_7_loss: 1.2766e-29 - model_6_loss: 0.0533 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.1309 - val_model_7_loss: 2.0509e-26 - val_model_6_loss: 0.8681 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 101/200\n",
      "675/675 [==============================] - 1s 966us/sample - loss: 0.0613 - model_7_loss: 4.4333e-28 - model_6_loss: 0.1194 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.0575 - val_model_7_loss: 2.5882e-25 - val_model_6_loss: 1.1450 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 102/200\n",
      "675/675 [==============================] - 1s 926us/sample - loss: 0.0709 - model_7_loss: 7.2210e-30 - model_6_loss: 0.1240 - model_7_acc: 1.0000 - model_6_acc: 0.9852 - val_loss: 1.2108 - val_model_7_loss: 1.0015e-23 - val_model_6_loss: 0.8464 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 103/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 940us/sample - loss: 0.0530 - model_7_loss: 2.4268e-29 - model_6_loss: 0.0376 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.2566 - val_model_7_loss: 1.5719e-23 - val_model_6_loss: 0.8691 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 104/200\n",
      "675/675 [==============================] - 1s 949us/sample - loss: 0.0482 - model_7_loss: 2.1628e-29 - model_6_loss: 0.0323 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.3158 - val_model_7_loss: 8.6218e-24 - val_model_6_loss: 1.5289 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 105/200\n",
      "675/675 [==============================] - 1s 934us/sample - loss: 0.0476 - model_7_loss: 1.0789e-28 - model_6_loss: 0.0345 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.2905 - val_model_7_loss: 4.0120e-24 - val_model_6_loss: 1.1111 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 106/200\n",
      "675/675 [==============================] - 1s 989us/sample - loss: 0.0465 - model_7_loss: 1.0364e-26 - model_6_loss: 0.0309 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.2941 - val_model_7_loss: 4.0860e-25 - val_model_6_loss: 1.5989 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 107/200\n",
      "675/675 [==============================] - 1s 989us/sample - loss: 0.0460 - model_7_loss: 5.9113e-27 - model_6_loss: 0.0303 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.2398 - val_model_7_loss: 7.2093e-25 - val_model_6_loss: 1.3612 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 108/200\n",
      "675/675 [==============================] - 1s 981us/sample - loss: 0.0591 - model_7_loss: 6.0452e-28 - model_6_loss: 0.1604 - model_7_acc: 1.0000 - model_6_acc: 0.9852 - val_loss: 1.1005 - val_model_7_loss: 7.9884e-29 - val_model_6_loss: 0.7590 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 109/200\n",
      "675/675 [==============================] - 1s 980us/sample - loss: 0.0579 - model_7_loss: 2.3090e-29 - model_6_loss: 0.0418 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.2461 - val_model_7_loss: 5.1011e-30 - val_model_6_loss: 1.3430 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 110/200\n",
      "675/675 [==============================] - 1s 1ms/sample - loss: 0.0574 - model_7_loss: 6.0093e-29 - model_6_loss: 0.0984 - model_7_acc: 1.0000 - model_6_acc: 0.9852 - val_loss: 1.3996 - val_model_7_loss: 5.1582e-30 - val_model_6_loss: 0.9680 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 111/200\n",
      "675/675 [==============================] - 1s 993us/sample - loss: 0.0494 - model_7_loss: 1.5934e-28 - model_6_loss: 0.0335 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.3199 - val_model_7_loss: 2.3887e-28 - val_model_6_loss: 1.5015 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 112/200\n",
      "675/675 [==============================] - 1s 1ms/sample - loss: 0.0413 - model_7_loss: 4.7009e-28 - model_6_loss: 0.0256 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.3354 - val_model_7_loss: 3.7057e-27 - val_model_6_loss: 1.6798 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 113/200\n",
      "675/675 [==============================] - 1s 990us/sample - loss: 0.0414 - model_7_loss: 7.5157e-29 - model_6_loss: 0.0263 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.3471 - val_model_7_loss: 1.8876e-29 - val_model_6_loss: 0.9305 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 114/200\n",
      "675/675 [==============================] - 1s 977us/sample - loss: 0.0321 - model_7_loss: 3.4958e-25 - model_6_loss: 0.0175 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.3894 - val_model_7_loss: 6.0657e-29 - val_model_6_loss: 0.9606 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 115/200\n",
      "675/675 [==============================] - 1s 974us/sample - loss: 0.0488 - model_7_loss: 3.2617e-28 - model_6_loss: 0.0327 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.3963 - val_model_7_loss: 9.3604e-29 - val_model_6_loss: 1.0151 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 116/200\n",
      "675/675 [==============================] - 1s 968us/sample - loss: 0.0644 - model_7_loss: 1.8644e-25 - model_6_loss: 0.1293 - model_7_acc: 1.0000 - model_6_acc: 0.9852 - val_loss: 1.0782 - val_model_7_loss: 1.3111e-28 - val_model_6_loss: 0.7459 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 117/200\n",
      "675/675 [==============================] - 1s 954us/sample - loss: 0.0617 - model_7_loss: 5.6808e-26 - model_6_loss: 0.0455 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.3092 - val_model_7_loss: 3.2218e-28 - val_model_6_loss: 1.4264 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 118/200\n",
      "675/675 [==============================] - 1s 984us/sample - loss: 0.0437 - model_7_loss: 6.3451e-27 - model_6_loss: 0.0280 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.3439 - val_model_7_loss: 1.9258e-28 - val_model_6_loss: 0.9287 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 119/200\n",
      "675/675 [==============================] - 1s 972us/sample - loss: 0.0446 - model_7_loss: 1.6047e-24 - model_6_loss: 0.0290 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.3215 - val_model_7_loss: 1.1902e-27 - val_model_6_loss: 1.4468 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 120/200\n",
      "675/675 [==============================] - 1s 971us/sample - loss: 0.0376 - model_7_loss: 5.2840e-28 - model_6_loss: 0.0226 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.4079 - val_model_7_loss: 6.0736e-28 - val_model_6_loss: 1.3600 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 121/200\n",
      "675/675 [==============================] - 1s 981us/sample - loss: 0.0349 - model_7_loss: 7.9586e-27 - model_6_loss: 0.0194 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.3756 - val_model_7_loss: 1.0084e-27 - val_model_6_loss: 1.4601 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 122/200\n",
      "675/675 [==============================] - 1s 982us/sample - loss: 0.0540 - model_7_loss: 1.7496e-26 - model_6_loss: 0.1854 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.0634 - val_model_7_loss: 6.6019e-30 - val_model_6_loss: 1.0946 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 123/200\n",
      "675/675 [==============================] - 1s 973us/sample - loss: 0.0861 - model_7_loss: 2.0867e-28 - model_6_loss: 0.0692 - model_7_acc: 1.0000 - model_6_acc: 0.9837 - val_loss: 1.2191 - val_model_7_loss: 2.9143e-30 - val_model_6_loss: 1.3091 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 124/200\n",
      "675/675 [==============================] - 1s 966us/sample - loss: 0.0385 - model_7_loss: 1.2024e-28 - model_6_loss: 0.0233 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.3210 - val_model_7_loss: 2.8960e-30 - val_model_6_loss: 1.5261 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 125/200\n",
      "675/675 [==============================] - 1s 969us/sample - loss: 0.0510 - model_7_loss: 9.6956e-26 - model_6_loss: 0.0649 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.1832 - val_model_7_loss: 1.8625e-29 - val_model_6_loss: 1.9066 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 126/200\n",
      "675/675 [==============================] - 1s 963us/sample - loss: 0.0445 - model_7_loss: 8.0764e-29 - model_6_loss: 0.0295 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.3841 - val_model_7_loss: 2.7057e-28 - val_model_6_loss: 1.5973 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 127/200\n",
      "675/675 [==============================] - 1s 946us/sample - loss: 0.0476 - model_7_loss: 4.5824e-29 - model_6_loss: 0.1146 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.1766 - val_model_7_loss: 2.1770e-29 - val_model_6_loss: 0.9279 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 128/200\n",
      "675/675 [==============================] - 1s 957us/sample - loss: 0.0699 - model_7_loss: 8.0607e-28 - model_6_loss: 0.0546 - model_7_acc: 1.0000 - model_6_acc: 0.9793 - val_loss: 1.3258 - val_model_7_loss: 8.0100e-29 - val_model_6_loss: 2.0885 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 129/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 941us/sample - loss: 0.0443 - model_7_loss: 1.1981e-26 - model_6_loss: 0.0823 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.1826 - val_model_7_loss: 1.8512e-25 - val_model_6_loss: 1.6021 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 130/200\n",
      "675/675 [==============================] - 1s 960us/sample - loss: 0.0419 - model_7_loss: 4.3179e-27 - model_6_loss: 0.0263 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.3833 - val_model_7_loss: 3.0057e-23 - val_model_6_loss: 1.3583 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 131/200\n",
      "675/675 [==============================] - 1s 946us/sample - loss: 0.0541 - model_7_loss: 6.7071e-27 - model_6_loss: 0.1221 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.4775 - val_model_7_loss: 1.5561e-24 - val_model_6_loss: 1.0250 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6716\n",
      "Epoch 132/200\n",
      "675/675 [==============================] - 1s 984us/sample - loss: 0.0534 - model_7_loss: 1.1197e-28 - model_6_loss: 0.0465 - model_7_acc: 1.0000 - model_6_acc: 0.9852 - val_loss: 1.4973 - val_model_7_loss: 4.5414e-27 - val_model_6_loss: 1.7402 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 133/200\n",
      "675/675 [==============================] - 1s 949us/sample - loss: 0.0391 - model_7_loss: 1.9362e-30 - model_6_loss: 0.0248 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.4934 - val_model_7_loss: 2.5893e-26 - val_model_6_loss: 1.0351 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 134/200\n",
      "675/675 [==============================] - 1s 967us/sample - loss: 0.0361 - model_7_loss: 1.3443e-30 - model_6_loss: 0.0220 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.5073 - val_model_7_loss: 4.8769e-28 - val_model_6_loss: 1.0452 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 135/200\n",
      "675/675 [==============================] - 1s 955us/sample - loss: 0.0517 - model_7_loss: 3.8852e-30 - model_6_loss: 0.1125 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.2557 - val_model_7_loss: 3.7651e-33 - val_model_6_loss: 0.8670 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7612\n",
      "Epoch 136/200\n",
      "675/675 [==============================] - 1s 972us/sample - loss: 0.0576 - model_7_loss: 6.1354e-29 - model_6_loss: 0.0486 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.3623 - val_model_7_loss: 1.3756e-32 - val_model_6_loss: 1.4876 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7463\n",
      "Epoch 137/200\n",
      "675/675 [==============================] - 1s 966us/sample - loss: 0.0481 - model_7_loss: 7.8173e-30 - model_6_loss: 0.0336 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.4065 - val_model_7_loss: 4.7741e-31 - val_model_6_loss: 0.9730 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 138/200\n",
      "675/675 [==============================] - 1s 992us/sample - loss: 0.0450 - model_7_loss: 6.4363e-30 - model_6_loss: 0.1265 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.2905 - val_model_7_loss: 1.9212e-34 - val_model_6_loss: 0.9727 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 139/200\n",
      "675/675 [==============================] - 1s 978us/sample - loss: 0.0525 - model_7_loss: 1.1416e-28 - model_6_loss: 0.0367 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.2984 - val_model_7_loss: 1.8364e-33 - val_model_6_loss: 1.4069 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 140/200\n",
      "675/675 [==============================] - 1s 963us/sample - loss: 0.0444 - model_7_loss: 2.4844e-29 - model_6_loss: 0.0306 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.3818 - val_model_7_loss: 2.9280e-31 - val_model_6_loss: 0.9590 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 141/200\n",
      "675/675 [==============================] - 1s 969us/sample - loss: 0.0418 - model_7_loss: 1.2662e-27 - model_6_loss: 0.0350 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.4835 - val_model_7_loss: 1.0611e-31 - val_model_6_loss: 1.0268 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 142/200\n",
      "675/675 [==============================] - 1s 987us/sample - loss: 0.0368 - model_7_loss: 4.5326e-27 - model_6_loss: 0.0221 - model_7_acc: 1.0000 - model_6_acc: 0.9956 - val_loss: 1.5149 - val_model_7_loss: 8.5153e-31 - val_model_6_loss: 1.0528 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 143/200\n",
      "675/675 [==============================] - 1s 963us/sample - loss: 0.0504 - model_7_loss: 3.6145e-28 - model_6_loss: 0.0344 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.4564 - val_model_7_loss: 4.7423e-31 - val_model_6_loss: 1.5406 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 144/200\n",
      "675/675 [==============================] - 1s 960us/sample - loss: 0.0339 - model_7_loss: 8.4184e-27 - model_6_loss: 0.0187 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.4960 - val_model_7_loss: 1.9917e-30 - val_model_6_loss: 1.0639 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 145/200\n",
      "675/675 [==============================] - 1s 1ms/sample - loss: 0.0452 - model_7_loss: 9.4958e-24 - model_6_loss: 0.0293 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.4515 - val_model_7_loss: 3.0487e-30 - val_model_6_loss: 1.0778 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 146/200\n",
      "675/675 [==============================] - 1s 961us/sample - loss: 0.0344 - model_7_loss: 1.7553e-27 - model_6_loss: 0.0193 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.4964 - val_model_7_loss: 1.1406e-30 - val_model_6_loss: 1.6013 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 147/200\n",
      "675/675 [==============================] - 1s 983us/sample - loss: 0.0334 - model_7_loss: 6.6286e-29 - model_6_loss: 0.0180 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.5409 - val_model_7_loss: 5.0311e-30 - val_model_6_loss: 3.1613 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 148/200\n",
      "675/675 [==============================] - 1s 970us/sample - loss: 0.0298 - model_7_loss: 6.0512e-29 - model_6_loss: 0.0151 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.6092 - val_model_7_loss: 1.2374e-29 - val_model_6_loss: 1.8055 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 149/200\n",
      "675/675 [==============================] - 1s 964us/sample - loss: 0.0354 - model_7_loss: 2.2443e-29 - model_6_loss: 0.0215 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.6129 - val_model_7_loss: 7.1355e-30 - val_model_6_loss: 1.4405 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 150/200\n",
      "675/675 [==============================] - 1s 967us/sample - loss: 0.0306 - model_7_loss: 9.2898e-29 - model_6_loss: 0.0153 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.6280 - val_model_7_loss: 1.2802e-29 - val_model_6_loss: 1.4865 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 151/200\n",
      "675/675 [==============================] - 1s 971us/sample - loss: 0.0351 - model_7_loss: 2.2271e-28 - model_6_loss: 0.0601 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.7550 - val_model_7_loss: 1.0556e-30 - val_model_6_loss: 1.7854 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6716\n",
      "Epoch 152/200\n",
      "675/675 [==============================] - 1s 988us/sample - loss: 0.0322 - model_7_loss: 1.0047e-28 - model_6_loss: 0.0173 - model_7_acc: 1.0000 - model_6_acc: 0.9956 - val_loss: 1.7843 - val_model_7_loss: 1.3660e-29 - val_model_6_loss: 1.4591 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 153/200\n",
      "675/675 [==============================] - 1s 964us/sample - loss: 0.0300 - model_7_loss: 2.3059e-28 - model_6_loss: 0.0156 - model_7_acc: 1.0000 - model_6_acc: 0.9970 - val_loss: 1.7521 - val_model_7_loss: 1.2510e-28 - val_model_6_loss: 2.0548 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 154/200\n",
      "675/675 [==============================] - 1s 974us/sample - loss: 0.0480 - model_7_loss: 3.9294e-28 - model_6_loss: 0.0319 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.6890 - val_model_7_loss: 4.4771e-30 - val_model_6_loss: 1.2033 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 985us/sample - loss: 0.0306 - model_7_loss: 6.3603e-26 - model_6_loss: 0.0155 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.7488 - val_model_7_loss: 2.2352e-29 - val_model_6_loss: 1.9353 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 156/200\n",
      "675/675 [==============================] - 1s 948us/sample - loss: 0.0372 - model_7_loss: 1.0246e-26 - model_6_loss: 0.0217 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.7571 - val_model_7_loss: 2.9934e-29 - val_model_6_loss: 2.0192 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 157/200\n",
      "675/675 [==============================] - 1s 958us/sample - loss: 0.0506 - model_7_loss: 1.6165e-27 - model_6_loss: 0.0354 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.5368 - val_model_7_loss: 1.6841e-28 - val_model_6_loss: 1.6998 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 158/200\n",
      "675/675 [==============================] - 1s 948us/sample - loss: 0.0353 - model_7_loss: 3.9843e-26 - model_6_loss: 0.0201 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.7141 - val_model_7_loss: 3.8799e-28 - val_model_6_loss: 1.1916 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 159/200\n",
      "675/675 [==============================] - 1s 983us/sample - loss: 0.0320 - model_7_loss: 4.7482e-27 - model_6_loss: 0.0175 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.7031 - val_model_7_loss: 2.9962e-27 - val_model_6_loss: 1.1823 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 160/200\n",
      "675/675 [==============================] - 1s 960us/sample - loss: 0.0482 - model_7_loss: 3.6040e-24 - model_6_loss: 0.1098 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.5213 - val_model_7_loss: 3.8063e-31 - val_model_6_loss: 1.7337 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 161/200\n",
      "675/675 [==============================] - 1s 1ms/sample - loss: 0.0500 - model_7_loss: 8.8767e-29 - model_6_loss: 0.0397 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.7715 - val_model_7_loss: 1.1567e-30 - val_model_6_loss: 2.9472 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 162/200\n",
      "675/675 [==============================] - 1s 950us/sample - loss: 0.0365 - model_7_loss: 1.9647e-28 - model_6_loss: 0.0212 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.7956 - val_model_7_loss: 4.7502e-29 - val_model_6_loss: 1.9146 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 163/200\n",
      "675/675 [==============================] - 1s 982us/sample - loss: 0.0323 - model_7_loss: 5.1042e-27 - model_6_loss: 0.0171 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.8014 - val_model_7_loss: 5.2487e-29 - val_model_6_loss: 1.2472 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 164/200\n",
      "675/675 [==============================] - 1s 970us/sample - loss: 0.0462 - model_7_loss: 5.6741e-28 - model_6_loss: 0.0302 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.7343 - val_model_7_loss: 1.9239e-29 - val_model_6_loss: 2.3672 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 165/200\n",
      "675/675 [==============================] - 1s 969us/sample - loss: 0.0338 - model_7_loss: 3.8196e-24 - model_6_loss: 0.0209 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.7151 - val_model_7_loss: 1.4586e-28 - val_model_6_loss: 1.1870 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 166/200\n",
      "675/675 [==============================] - 1s 989us/sample - loss: 0.0389 - model_7_loss: 9.8035e-25 - model_6_loss: 0.0232 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.5269 - val_model_7_loss: 1.9869e-28 - val_model_6_loss: 1.0737 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 167/200\n",
      "675/675 [==============================] - 1s 973us/sample - loss: 0.0536 - model_7_loss: 1.0513e-25 - model_6_loss: 0.1410 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.2862 - val_model_7_loss: 7.6650e-33 - val_model_6_loss: 0.8991 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 168/200\n",
      "675/675 [==============================] - 1s 984us/sample - loss: 0.0649 - model_7_loss: 1.6159e-26 - model_6_loss: 0.1319 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.0740 - val_model_7_loss: 5.6075e-27 - val_model_6_loss: 1.6061 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7761\n",
      "Epoch 169/200\n",
      "675/675 [==============================] - 1s 961us/sample - loss: 0.0536 - model_7_loss: 1.4826e-23 - model_6_loss: 0.0381 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.1854 - val_model_7_loss: 3.1989e-27 - val_model_6_loss: 0.8185 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 170/200\n",
      "675/675 [==============================] - 1s 995us/sample - loss: 0.0392 - model_7_loss: 7.4061e-24 - model_6_loss: 0.0444 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.2743 - val_model_7_loss: 1.4840e-28 - val_model_6_loss: 1.0187 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 171/200\n",
      "675/675 [==============================] - 1s 986us/sample - loss: 0.0439 - model_7_loss: 8.2912e-25 - model_6_loss: 0.0293 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.3195 - val_model_7_loss: 3.9566e-27 - val_model_6_loss: 1.6068 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 172/200\n",
      "675/675 [==============================] - 1s 988us/sample - loss: 0.0434 - model_7_loss: 2.0214e-23 - model_6_loss: 0.0424 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.4359 - val_model_7_loss: 1.3841e-27 - val_model_6_loss: 1.0820 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6716\n",
      "Epoch 173/200\n",
      "675/675 [==============================] - 1s 969us/sample - loss: 0.0382 - model_7_loss: 1.5274e-23 - model_6_loss: 0.0250 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.4670 - val_model_7_loss: 8.8224e-25 - val_model_6_loss: 1.0137 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 174/200\n",
      "675/675 [==============================] - 1s 984us/sample - loss: 0.0474 - model_7_loss: 3.2948e-24 - model_6_loss: 0.0318 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.4611 - val_model_7_loss: 5.3796e-24 - val_model_6_loss: 1.6594 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 175/200\n",
      "675/675 [==============================] - 1s 972us/sample - loss: 0.0485 - model_7_loss: 2.6630e-23 - model_6_loss: 0.0353 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.3926 - val_model_7_loss: 1.6757e-23 - val_model_6_loss: 1.6591 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 176/200\n",
      "675/675 [==============================] - 1s 1ms/sample - loss: 0.0316 - model_7_loss: 1.8309e-23 - model_6_loss: 0.0162 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.4588 - val_model_7_loss: 4.4892e-23 - val_model_6_loss: 1.0079 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 177/200\n",
      "675/675 [==============================] - 1s 975us/sample - loss: 0.0365 - model_7_loss: 2.2111e-23 - model_6_loss: 0.0209 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.4731 - val_model_7_loss: 4.1883e-25 - val_model_6_loss: 1.7024 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 178/200\n",
      "675/675 [==============================] - 1s 990us/sample - loss: 0.0388 - model_7_loss: 2.6443e-24 - model_6_loss: 0.0944 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.3396 - val_model_7_loss: 3.8410e-29 - val_model_6_loss: 0.9758 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 179/200\n",
      "675/675 [==============================] - 1s 988us/sample - loss: 0.0591 - model_7_loss: 3.5130e-23 - model_6_loss: 0.0446 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.4163 - val_model_7_loss: 1.9619e-26 - val_model_6_loss: 1.0426 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 180/200\n",
      "675/675 [==============================] - 1s 986us/sample - loss: 0.0345 - model_7_loss: 1.0027e-22 - model_6_loss: 0.0654 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.5212 - val_model_7_loss: 2.1117e-28 - val_model_6_loss: 1.9365 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6716\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 997us/sample - loss: 0.0497 - model_7_loss: 1.4823e-24 - model_6_loss: 0.0337 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.5219 - val_model_7_loss: 2.4322e-26 - val_model_6_loss: 1.0564 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 182/200\n",
      "675/675 [==============================] - 1s 1ms/sample - loss: 0.0297 - model_7_loss: 6.8942e-25 - model_6_loss: 0.0215 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.6169 - val_model_7_loss: 6.4581e-27 - val_model_6_loss: 1.1187 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 183/200\n",
      "675/675 [==============================] - 1s 977us/sample - loss: 0.0422 - model_7_loss: 5.9763e-26 - model_6_loss: 0.0408 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.4818 - val_model_7_loss: 2.0008e-26 - val_model_6_loss: 1.0250 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 184/200\n",
      "675/675 [==============================] - 1s 988us/sample - loss: 0.0402 - model_7_loss: 9.3634e-24 - model_6_loss: 0.0246 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.4716 - val_model_7_loss: 2.8637e-26 - val_model_6_loss: 1.3354 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7463\n",
      "Epoch 185/200\n",
      "675/675 [==============================] - 1s 972us/sample - loss: 0.0334 - model_7_loss: 2.4245e-26 - model_6_loss: 0.0179 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.5685 - val_model_7_loss: 1.1238e-26 - val_model_6_loss: 1.0846 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7463\n",
      "Epoch 186/200\n",
      "675/675 [==============================] - 1s 989us/sample - loss: 0.0435 - model_7_loss: 2.6302e-26 - model_6_loss: 0.0679 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.4636 - val_model_7_loss: 3.2838e-27 - val_model_6_loss: 1.0114 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 187/200\n",
      "675/675 [==============================] - 1s 963us/sample - loss: 0.0559 - model_7_loss: 2.3083e-27 - model_6_loss: 0.0425 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.3827 - val_model_7_loss: 7.8950e-24 - val_model_6_loss: 1.7500 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 188/200\n",
      "675/675 [==============================] - 1s 955us/sample - loss: 0.0383 - model_7_loss: 3.4682e-27 - model_6_loss: 0.0229 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.4659 - val_model_7_loss: 1.4377e-25 - val_model_6_loss: 1.7830 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 189/200\n",
      "675/675 [==============================] - 1s 976us/sample - loss: 0.0296 - model_7_loss: 3.3741e-25 - model_6_loss: 0.0148 - model_7_acc: 1.0000 - model_6_acc: 0.9941 - val_loss: 1.5611 - val_model_7_loss: 2.9552e-26 - val_model_6_loss: 1.6089 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7164\n",
      "Epoch 190/200\n",
      "675/675 [==============================] - 1s 966us/sample - loss: 0.0296 - model_7_loss: 1.8834e-26 - model_6_loss: 0.0143 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.7022 - val_model_7_loss: 2.8373e-26 - val_model_6_loss: 1.8635 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 191/200\n",
      "675/675 [==============================] - 1s 990us/sample - loss: 0.0431 - model_7_loss: 5.0659e-20 - model_6_loss: 0.0274 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.6792 - val_model_7_loss: 1.7543e-26 - val_model_6_loss: 1.6445 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 192/200\n",
      "675/675 [==============================] - 1s 982us/sample - loss: 0.0433 - model_7_loss: 8.0383e-23 - model_6_loss: 0.0498 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.4194 - val_model_7_loss: 1.2941e-25 - val_model_6_loss: 1.8905 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7313\n",
      "Epoch 193/200\n",
      "675/675 [==============================] - 1s 976us/sample - loss: 0.0698 - model_7_loss: 3.8036e-23 - model_6_loss: 0.0529 - model_7_acc: 1.0000 - model_6_acc: 0.9807 - val_loss: 1.5960 - val_model_7_loss: 6.2676e-27 - val_model_6_loss: 1.1047 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 194/200\n",
      "675/675 [==============================] - 1s 989us/sample - loss: 0.0614 - model_7_loss: 7.5811e-25 - model_6_loss: 0.0449 - model_7_acc: 1.0000 - model_6_acc: 0.9822 - val_loss: 1.4349 - val_model_7_loss: 8.7138e-27 - val_model_6_loss: 0.9914 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 195/200\n",
      "675/675 [==============================] - 1s 978us/sample - loss: 0.0425 - model_7_loss: 5.3236e-21 - model_6_loss: 0.0340 - model_7_acc: 1.0000 - model_6_acc: 0.9881 - val_loss: 1.4654 - val_model_7_loss: 6.0367e-27 - val_model_6_loss: 1.3185 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.7015\n",
      "Epoch 196/200\n",
      "675/675 [==============================] - 1s 959us/sample - loss: 0.0368 - model_7_loss: 1.2092e-25 - model_6_loss: 0.0229 - model_7_acc: 1.0000 - model_6_acc: 0.9896 - val_loss: 1.4508 - val_model_7_loss: 7.8385e-26 - val_model_6_loss: 2.4347 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 197/200\n",
      "675/675 [==============================] - 1s 1ms/sample - loss: 0.0586 - model_7_loss: 1.3218e-26 - model_6_loss: 0.0431 - model_7_acc: 1.0000 - model_6_acc: 0.9867 - val_loss: 1.6196 - val_model_7_loss: 1.0556e-25 - val_model_6_loss: 1.8812 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 198/200\n",
      "675/675 [==============================] - 1s 973us/sample - loss: 0.0342 - model_7_loss: 2.9684e-22 - model_6_loss: 0.0189 - model_7_acc: 1.0000 - model_6_acc: 0.9926 - val_loss: 1.6082 - val_model_7_loss: 4.5740e-26 - val_model_6_loss: 1.1512 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 199/200\n",
      "675/675 [==============================] - 1s 987us/sample - loss: 0.0332 - model_7_loss: 7.1487e-22 - model_6_loss: 0.0178 - model_7_acc: 1.0000 - model_6_acc: 0.9911 - val_loss: 1.6187 - val_model_7_loss: 1.1280e-25 - val_model_6_loss: 1.5012 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Epoch 200/200\n",
      "675/675 [==============================] - 1s 980us/sample - loss: 0.0268 - model_7_loss: 1.2364e-22 - model_6_loss: 0.0116 - model_7_acc: 1.0000 - model_6_acc: 0.9956 - val_loss: 1.6511 - val_model_7_loss: 5.9115e-25 - val_model_6_loss: 1.1425 - val_model_7_acc: 1.0000 - val_model_6_acc: 0.6866\n",
      "Train on 1350 samples, validate on 67 samples\n",
      "1350/1350 [==============================] - 0s 72us/sample - loss: 0.0318 - acc: 0.9933 - val_loss: 0.0605 - val_acc: 0.9851\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "mcp = ModelCheckpoint(\"q.hf5\", monitor='val_bacc', save_weights_only=True, save_best_only=True)\n",
    "\n",
    "for i in range(100):\n",
    "    Q.fit_generator(datagen.flow(trainX, trainY, batch_size=BATCH_SIZE), epochs=100,  shuffle=True, \n",
    "                    validation_data=(testX, testY), callbacks=[mcp])\n",
    "    \n",
    "    Q.save_weights(\"qb.hf5\")\n",
    "    \n",
    "    Q.load_weights(\"q.hf5\")\n",
    "    func = K.function(Q.input, Q.layers[-2].output)\n",
    "    Gtrain, Gval = func(trainX), func(testX) \n",
    "    \n",
    "    gan = define_gan(G, D, Q)\n",
    "    Q.trainable = True\n",
    "    \n",
    "    gan.fit(Gtrain, [np.zeros((Gtrain.shape[0],)), trainY], epochs=100,\n",
    "            validation_data=(Gval, [np.zeros((Gval.shape[0],)), testY]))\n",
    "    \n",
    "    fake = K.function(gan.input, G.output)(Gtrain)\n",
    "    Dx = np.append(trainX, fake, axis=0)\n",
    "    Dy = np.append(np.ones((trainX.shape[0],)), np.zeros((fake.shape[0],)))\n",
    "    \n",
    "    s = np.arange(Dx.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    Dx, Dy = Dx[s], Dy[s]\n",
    "    D.fit(Dx, Dy, batch_size=BATCH_SIZE, validation_data=(testX, np.ones((testX.shape[0],))))\n",
    "\n",
    "    Q.load_weights(\"qb.hf5\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Assignment-19.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

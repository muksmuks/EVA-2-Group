{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "GVY3XJhuhvkg",
    "outputId": "3344865b-7c31-4282-d6cb-882af5de3e2e"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    os.chdir('/content')\n",
    "    if not os.path.isdir('/content/EVA-2-Group/'):\n",
    "        !git clone https://github.com/sambitdash/EVA-2-Group.git\n",
    "    os.chdir('/content/EVA-2-Group/Session-19')\n",
    "    !pwd\n",
    "    \n",
    "    !git config user.email \"sambitdash@gmail.com\"\n",
    "    !git config user.name \"Sambit Kumar Dash\"\n",
    "    !git config user.password \"your password\"\n",
    "    !git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "BbhSFX9Jhvkl",
    "outputId": "c4a40efa-9706-4279-f048-5bebb513c149"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "car_types = ['hatch', 'sedan', 'suv']\n",
    "\n",
    "def resize_image(img, size=(64,64)):\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    if h == w: \n",
    "        return cv2.resize(img, size, cv2.INTER_AREA)\n",
    "\n",
    "    dif = h if h > w else w\n",
    "\n",
    "    interpolation = cv2.INTER_AREA if dif > (size[0]+size[1])//2 else cv2.INTER_CUBIC\n",
    "\n",
    "    x_pos = (dif - w)//2\n",
    "    y_pos = (dif - h)//2\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        mask = np.zeros((dif, dif), dtype=img.dtype)\n",
    "        mask[y_pos:y_pos+h, x_pos:x_pos+w] = img[:h, :w]\n",
    "    else:\n",
    "        c = img.shape[2]\n",
    "        mask = np.zeros((dif, dif, c), dtype=img.dtype)\n",
    "        mask[y_pos:y_pos+h, x_pos:x_pos+w, :] = img[:h, :w, :]\n",
    "\n",
    "    return cv2.resize(mask, size, interpolation)\n",
    "\n",
    "spath, dpath = join('data', 'cars'), join('data', 'norm')\n",
    "\n",
    "if not os.path.isdir(dpath):\n",
    "    os.mkdir(dpath)\n",
    "\n",
    "imgs = {}\n",
    "\n",
    "\n",
    "for ct in car_types:\n",
    "    sp, dp = join(spath, ct), join(dpath, ct)\n",
    "    alen = 1024\n",
    "    imgs[ct] = np.zeros((1024, 64, 64, 3))\n",
    "    if not os.path.isdir(dp):\n",
    "        os.mkdir(dp)\n",
    "    tlen = 0\n",
    "    for f in listdir(sp):\n",
    "        sf, df = join(sp, f), join(dp, f)\n",
    "        img = cv2.imread(sf)\n",
    "        img = resize_image(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        tlen += 1\n",
    "        if tlen > alen:\n",
    "            imgs[ct] = np.append(imgs[ct], np.zeros((1024, 64, 64, 3)))\n",
    "            alen += 1024\n",
    "        imgs[ct][tlen-1] = img\n",
    "    imgs[ct] = imgs[ct][:tlen]\n",
    "    print(imgs[ct].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "xgWY9ontMgFW",
    "outputId": "1b1466f1-116e-441b-9ccf-d19252b1719e"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 1.x\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)\n",
    "\n",
    "from tensorflow.keras import utils \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "trainx, testx = imgs['hatch'][:300,:,:,:], imgs['hatch'][300:,:,:,:]\n",
    "\n",
    "trainy, testy = np.zeros(trainx.shape[0], dtype=float), np.zeros(testx.shape[0], dtype=float)\n",
    "\n",
    "trainx = np.append(trainx, imgs['sedan'][:375,:,:,:], axis=0)\n",
    "testx = np.append(testx, imgs['sedan'][375:,:,:,:], axis=0)\n",
    "\n",
    "ltrain, ltest = trainx.shape[0] - trainy.shape[0], testx.shape[0] - testy.shape[0]\n",
    "\n",
    "trainy, testy = np.append(trainy, np.ones(ltrain, dtype=float)), np.append(testy, np.ones(ltest, dtype=float))\n",
    "\n",
    "\n",
    "print(trainx.shape, \"y\", trainy.shape)\n",
    "print(testx.shape, 'y', testy.shape)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "datagen.fit(trainx)\n",
    "\n",
    "trainX, trainY = trainx, trainy\n",
    "testX, testY   = testx, testy\n",
    "\n",
    "for i in range(0):\n",
    "    iterate = datagen.flow(trainx, trainy, batch_size=len(trainx), shuffle=True)\n",
    "    x, y = iterate.next()\n",
    "    trainX, trainY = np.append(trainX, x, axis=0), np.append(trainY, y, axis=0)\n",
    "\n",
    "for i in range(0):\n",
    "    iterate = datagen.flow(testx, testy, batch_size=len(testx), shuffle=True)\n",
    "    x, y = iterate.next()\n",
    "    testX, testY = np.append(testX, x, axis=0), np.append(testY, y, axis=0)\n",
    "\n",
    "\n",
    "print(trainX.shape, \"y\", trainY.shape)\n",
    "print(testX.shape, \"y\", testY.shape)\n",
    "\n",
    "trainx, trainy = trainX, trainY\n",
    "testx, testy   = testX, testY\n",
    "\n",
    "trainx = trainx.astype('float32') / 255\n",
    "testx  = testx.astype('float32') / 255\n",
    "\n",
    "trainx_mean = np.mean(trainx, axis=(0, 1, 2))\n",
    "trainx_std  = np.std(trainx, axis=(0, 1, 2))\n",
    "\n",
    "print(trainx_mean, trainx_std)\n",
    "\n",
    "trainx -= trainx_mean\n",
    "trainx /= trainx_std\n",
    "\n",
    "testx -= trainx_mean\n",
    "testx /= trainx_std\n",
    "\n",
    "trainX, trainY = trainx, trainy #utils.to_binary(trainy)\n",
    "testX,  testY  = testx,  testy  #utils.to_binary(testy)\n",
    "\n",
    "min_pix, max_pix = trainX.min(), trainX.max()\n",
    "\n",
    "print(min_pix, max_pix)\n",
    "print(testX.min(), testX.max())\n",
    "\n",
    "print(trainX.shape, trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "K-vi5P3dSZBh",
    "outputId": "78a61916-8818-44c5-8ddf-9c2a7db96b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ik_4kxmzJGoo"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, Activation, MaxPool2D\n",
    "from tensorflow.keras.layers import add, Input, Dense, Flatten, GlobalAvgPool2D, GlobalAvgPool1D\n",
    "from tensorflow.keras.initializers import zeros\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def ResConv(x, kernel=(3, 3), depth=32, maxpool=False):\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    if maxpool :\n",
    "        x = MaxPool2D()(x)\n",
    "    x = Conv2D(depth, kernel, padding='same', use_bias=False)(x)\n",
    "    return x\n",
    "\n",
    "def ResUnit(x, depth=32, maxpool=False):\n",
    "    x = ResConv(x, depth=depth, maxpool=maxpool)\n",
    "    x = ResConv(x, depth=depth)\n",
    "    return x\n",
    "    \n",
    "def ResNetBlock(x, nunit, depth=32, maxpool=False):\n",
    "    assert nunit > 0, \"Ensure there are at least 1 unit in the ResNet Block\"\n",
    "    nunit -= 1\n",
    "    if maxpool:\n",
    "        xskip = Conv2D(depth, (1, 1), strides=2, use_bias=False)(x)\n",
    "    else: \n",
    "        xskip = x\n",
    "    x = add([ResUnit(x, depth=depth, maxpool=maxpool), xskip])\n",
    "    if nunit >= 1:\n",
    "        nunit -= 1\n",
    "        for i in range(nunit):\n",
    "            x = add([ResUnit(x, depth=depth), x])\n",
    "        x = add([ResUnit(x, depth=depth), x])\n",
    "    return x\n",
    "\n",
    "# Returns latent vector of 32 bytes\n",
    "def ResNet9(x):\n",
    "    x = Conv2D(64, (7, 7), strides=2, padding='same', use_bias=False)(x)\n",
    "    x = MaxPool2D((3, 3),  strides=2, padding='same')(x)\n",
    "    \n",
    "    nunits   = (2, 3, 2)\n",
    "    maxpools = (False, True, True)\n",
    "    depths   = (64, 32, 32)\n",
    "    \n",
    "    for i in range(3):\n",
    "        x = ResNetBlock(x, nunits[i], depth=depths[i], maxpool=maxpools[i])\n",
    "    x = GlobalAvgPool2D()(x)\n",
    "    return x\n",
    "\n",
    "#def D_init():\n",
    "#    xin = Input(shape=(64, 64, 3))\n",
    "#    x = ResNet9(xin)\n",
    "#    x = Dense(1, use_bias=False, activation='sigmoid')(x)\n",
    "#    return Model(xin, x)\n",
    "\n",
    "def Q_init():\n",
    "    xin = Input(shape=(64, 64, 3))\n",
    "    x = ResNet9(xin)\n",
    "    x = Dense(1, use_bias=False, activation='sigmoid')(x)\n",
    "    return Model(xin, x)\n",
    "\n",
    "\n",
    "def D_init():\n",
    "    xin = Input(shape=(64, 64, 3))\n",
    "    x = ResNet9(xin)\n",
    "    x = Dense(1, use_bias=False, activation='sigmoid')(x)\n",
    "    return Model(xin, x)\n",
    "\n",
    "\n",
    "#    xin = Input(shape=(64, 64, 3))\n",
    "#    x = Conv2D(8,(7, 7), use_bias=False, kernel_regularizer=regularizers.l2(0.01), padding='same')(xin)\n",
    "#    x = BatchNormalization()(x)\n",
    "#    x = Activation('relu')(x)\n",
    "#    x = MaxPool2D(2,2)(x)\n",
    "#    x = Conv2D(8,(7, 7), use_bias=False, kernel_regularizer=regularizers.l2(0.01), padding='same')(x)\n",
    "#    x = BatchNormalization()(x)\n",
    "#    x = Activation('relu')(x)\n",
    "#    x = MaxPool2D(2,2)(x)\n",
    "#    x = Conv2D(1,(7, 7), use_bias=False, kernel_regularizer=regularizers.l2(0.01), padding='same')(x)\n",
    "#    x = BatchNormalization()(x)\n",
    "#    x = Activation('relu')(x)\n",
    "#    x = GlobalAvgPool2D()(x)\n",
    "#    x = Flatten()(x)\n",
    "#    x = Activation('sigmoid')(x)\n",
    "#    return Model(xin, x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import UpSampling2D, Reshape, Conv2DTranspose\n",
    "\n",
    "def InvResConv(x, kernel=(3, 3), depth=32, upscale=False):\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    if upscale :\n",
    "        x = UpSampling2D((2,2))(x)\n",
    "    x = Conv2D(depth, kernel, padding='same', use_bias=False)(x)\n",
    "    return x\n",
    "\n",
    "def InvResUnit(x, depth=32, upscale=False):\n",
    "    x = InvResConv(x, depth=depth)\n",
    "    x = InvResConv(x, depth=depth, upscale=upscale)\n",
    "    return x\n",
    "    \n",
    "def InvResNetBlock(x, nunit, depth=32, upscale=False):\n",
    "    assert nunit > 0, \"Ensure there are at least 1 unit in the ResNet Block\"\n",
    "    nunit -= 1\n",
    "    x = InvResUnit(x, depth=depth, upscale=upscale)\n",
    "    if nunit >= 1:\n",
    "        nunit -= 1\n",
    "        for i in range(nunit):\n",
    "            x = InvResUnit(x, depth=depth)\n",
    "        x = InvResUnit(x, depth=depth)\n",
    "    return x\n",
    "\n",
    "\n",
    "def InvResNet9(x, size=(4,4)):\n",
    "    depths   = (32, 32, 64)\n",
    "    upscales = (True, True, False)\n",
    "    nunits   = (2, 3, 2)\n",
    "\n",
    "    x = UpSampling2D(size)(x)\n",
    "    for i in range(3):\n",
    "        x = InvResNetBlock(x, nunits[i], depth=depths[i], upscale=upscales[i])\n",
    "    x = UpSampling2D((2,2))(x)\n",
    "    x = Conv2DTranspose(3, (7, 7), strides=2, padding='same', use_bias=False)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_init():\n",
    "    xin = Input(shape=(32,), name=\"Input\")\n",
    "    x = Reshape((1,1,32))(xin)\n",
    "    x = InvResNet9(x)\n",
    "    x = Activation('relu')(x)\n",
    "    model = Model(xin, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPClHV_yKfSx"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        img_h, img_w, img_c = input_img.shape\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = 8 #int(np.sqrt(s / r))\n",
    "            h = 8 #int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "        else:\n",
    "            c = 0.0 #np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w, :] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        fill_mode = 'constant',\n",
    "        cval=0,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=8,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=8,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        preprocessing_function=get_random_eraser(v_l=min_pix, v_h=max_pix, pixel_level=False)\n",
    ")\n",
    "datagen.fit(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzXg6VXY0fbU"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def accuracy(test_x, test_y, model):\n",
    "    result = model.predict(test_x)\n",
    "    predicted_class = np.argmax(result)\n",
    "    true_class = np.argmax(test_y)\n",
    "    num_correct = np.sum(predicted_class == true_class) \n",
    "    accuracy = float(num_correct)/result.shape[0]\n",
    "    return (accuracy * 100)\n",
    "\n",
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['bacc'])+1),model_history.history['bacc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_bacc'])+1),model_history.history['val_bacc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['bacc'])+1),len(model_history.history['bacc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sambit/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/sambit/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 64)   9408        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 16, 16, 64)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16, 16, 64)   256         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16, 16, 64)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 64)   36864       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16, 16, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16, 16, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 64)   36864       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 16, 16, 64)   0           conv2d_2[0][0]                   \n",
      "                                                                 max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 64)   256         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 64)   36864       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16, 16, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 64)   36864       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 16, 64)   0           conv2d_4[0][0]                   \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 64)   256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 64)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 8, 8, 32)     18432       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 8, 8, 32)     128         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 8, 8, 32)     0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 8, 8, 32)     9216        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 8, 8, 32)     2048        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 8, 8, 32)     0           conv2d_7[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 8, 8, 32)     128         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 8, 8, 32)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 32)     9216        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 8, 8, 32)     128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 8, 8, 32)     0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 32)     9216        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 8, 8, 32)     0           conv2d_9[0][0]                   \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 32)     128         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 32)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 8, 32)     9216        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 32)     128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 32)     0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 8, 8, 32)     9216        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 8, 8, 32)     0           conv2d_11[0][0]                  \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 8, 8, 32)     128         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 8, 8, 32)     0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 32)     0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 4, 4, 32)     9216        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 4, 4, 32)     128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 4, 4, 32)     0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 4, 4, 32)     9216        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 4, 4, 32)     1024        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 4, 4, 32)     0           conv2d_14[0][0]                  \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 4, 4, 32)     128         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 4, 4, 32)     0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 4, 4, 32)     9216        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 4, 4, 32)     128         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 4, 4, 32)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 4, 4, 32)     9216        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 4, 4, 32)     0           conv2d_16[0][0]                  \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 32)           0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            32          global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 263,776\n",
      "Trainable params: 262,560\n",
      "Non-trainable params: 1,216\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "WT_DECAY   = 1e-5\n",
    "MOMENTUM   = 0.90\n",
    "LEARNING_RATE = 0.002\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy, binary_crossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def l2_weights(model):\n",
    "    l2 = 0\n",
    "    for layer in model.layers: \n",
    "        if isinstance(layer, Model):\n",
    "            continue\n",
    "        wt = layer.weights\n",
    "        if len(wt) > 0:\n",
    "            l2 += K.sum(K.pow(wt, 2))\n",
    "    return l2\n",
    "\n",
    "def reg_loss(model):\n",
    "    def rloss(y_true, y_pred):\n",
    "        return WT_DECAY*l2_weights(model)\n",
    "    return rloss\n",
    "\n",
    "def loss_with_regularization(model):\n",
    "    def loss(y_true, y_pred):\n",
    "        return binary_crossentropy(y_true, y_pred, True) + reg_loss(model)(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def bacc(y_true, y_pred):\n",
    "    y_pred = K.cast(y_pred > 0.5, dtype=y_true.dtype)\n",
    "    return K.mean(K.equal(y_true, y_pred), axis=-1)\n",
    "\n",
    "Q = Q_init()\n",
    "optimizer = SGD(lr=LEARNING_RATE, momentum=MOMENTUM, nesterov=True)\n",
    "Q.compile(optimizer=optimizer, loss=loss_with_regularization(Q), metrics=[bacc, reg_loss(Q)])\n",
    "Q.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 64)   9408        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 64)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 64)   36864       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 64)   256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 64)   36864       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 16, 64)   0           conv2d_19[0][0]                  \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 64)   256         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 64)   36864       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 64)   256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 64)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 64)   36864       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 64)   0           conv2d_21[0][0]                  \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 64)   256         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 64)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 8, 8, 32)     18432       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 32)     128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 32)     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 8, 8, 32)     9216        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 32)     2048        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 32)     0           conv2d_24[0][0]                  \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 32)     128         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 32)     0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 8, 8, 32)     9216        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 32)     128         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 32)     0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 32)     9216        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 8, 8, 32)     0           conv2d_26[0][0]                  \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 32)     128         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 32)     0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 32)     9216        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 32)     128         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 8, 32)     0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 32)     9216        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 8, 8, 32)     0           conv2d_28[0][0]                  \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 8, 32)     128         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 8, 32)     0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 4, 4, 32)     0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 4, 4, 32)     9216        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 4, 4, 32)     128         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 4, 4, 32)     0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 4, 4, 32)     9216        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 4, 4, 32)     1024        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 4, 4, 32)     0           conv2d_31[0][0]                  \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 4, 4, 32)     128         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 4, 4, 32)     0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 4, 4, 32)     9216        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 4, 4, 32)     128         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 4, 4, 32)     0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 4, 4, 32)     9216        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 4, 4, 32)     0           conv2d_33[0][0]                  \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 32)           0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            32          global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 263,776\n",
      "Trainable params: 262,560\n",
      "Non-trainable params: 1,216\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "D = D_init()\n",
    "D.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 4, 4, 32)          9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 8, 8, 32)          9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 8, 8, 32)          9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 8, 8, 32)          9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 8, 8, 32)          9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 16, 16, 32)        9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 16, 16, 32)        9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 16, 16, 32)        9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 16, 16, 32)        9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 16, 16, 32)        9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 16, 16, 64)        18432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 16, 16, 64)        36864     \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 16, 16, 64)        36864     \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 16, 16, 64)        36864     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 64, 64, 3)         9408      \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 64, 64, 3)         0         \n",
      "=================================================================\n",
      "Total params: 232,768\n",
      "Trainable params: 231,680\n",
      "Non-trainable params: 1,088\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "G = G_init()\n",
    "\n",
    "G.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_loss(model):\n",
    "    def gloss(y_true, y_pred):\n",
    "        return binary_crossentropy(y_true, y_pred) + 5e-3*l2_weights(model)\n",
    "    return gloss\n",
    "    \n",
    "def define_gan(g_model, d_model, q_model):\n",
    "    d_model.trainable = False\n",
    "    q_model.trainable = False\n",
    "    d_output = d_model(g_model.output)\n",
    "    q_output = q_model(g_model.output)\n",
    "    gan = Model(g_model.input, [d_output, q_output])\n",
    "    #opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    gan.compile(loss=[gan_loss(gan), 'binary_crossentropy'], optimizer='adam', metrics=['acc'])\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Count #####  0\n",
      "67/67 [==============================] - 0s 270us/sample - loss: 0.6991 - bacc: 0.7015 - rloss: 0.0243\n",
      "[0.6990796243076893, 0.70149255, 0.02429983]\n",
      "1349/1349 [==============================] - 0s 268us/sample - loss: 0.7869 - acc: 0.2728\n",
      "[0.7868572477060747, 0.27279466]\n",
      "1349/1349 [==============================] - 0s 147us/sample - loss: 2.9041 - acc: 0.4996\n",
      "[2.904079213152999, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 148us/sample - loss: 4.6483 - acc: 0.4996\n",
      "[4.64829337358298, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 152us/sample - loss: 6.6667 - acc: 0.4996\n",
      "[6.666700429965868, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 148us/sample - loss: 8.8298 - acc: 0.4996\n",
      "[8.82981691438238, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 139us/sample - loss: 11.0557 - acc: 0.4996\n",
      "[11.05569741423701, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 145us/sample - loss: 13.3373 - acc: 0.4996\n",
      "[13.337340755759564, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 149us/sample - loss: 15.5007 - acc: 0.4996\n",
      "[15.500672185571217, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 149us/sample - loss: 17.8160 - acc: 0.4996\n",
      "[17.816021197455473, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 145us/sample - loss: 19.7820 - acc: 0.4996\n",
      "[19.781993441090396, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 156us/sample - loss: 21.4668 - acc: 0.4996\n",
      "[21.466839738736955, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 146us/sample - loss: 22.6446 - acc: 0.4996\n",
      "[22.6446065499749, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 152us/sample - loss: 23.8565 - acc: 0.4996\n",
      "[23.856454870451635, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 149us/sample - loss: 24.6775 - acc: 0.4996\n",
      "[24.67745611842603, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 151us/sample - loss: 25.2983 - acc: 0.4996\n",
      "[25.298261980377717, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 147us/sample - loss: 25.7518 - acc: 0.4996\n",
      "[25.751786152109382, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 143us/sample - loss: 25.9578 - acc: 0.4996\n",
      "[25.95784632270649, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 143us/sample - loss: 25.8148 - acc: 0.4996\n",
      "[25.814825121608816, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 149us/sample - loss: 25.4568 - acc: 0.4996\n",
      "[25.456798013357695, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 141us/sample - loss: 25.2152 - acc: 0.4996\n",
      "[25.215155825604324, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 147us/sample - loss: 24.2075 - acc: 0.4996\n",
      "[24.20752399598695, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 147us/sample - loss: 23.6538 - acc: 0.4996\n",
      "[23.653771122620668, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 150us/sample - loss: 22.8919 - acc: 0.4996\n",
      "[22.891882774828098, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 146us/sample - loss: 21.9796 - acc: 0.4996\n",
      "[21.97959486902688, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 148us/sample - loss: 21.1543 - acc: 0.4996\n",
      "[21.15425674503516, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 145us/sample - loss: 20.2156 - acc: 0.4996\n",
      "[20.215640533933648, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 145us/sample - loss: 19.0556 - acc: 0.4996\n",
      "[19.05561239724693, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 149us/sample - loss: 17.9537 - acc: 0.4996\n",
      "[17.9537232146606, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 148us/sample - loss: 16.8095 - acc: 0.4996\n",
      "[16.809463515115546, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 146us/sample - loss: 15.7580 - acc: 0.4996\n",
      "[15.757976459873616, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 144us/sample - loss: 14.6394 - acc: 0.4996\n",
      "[14.639442973264153, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 147us/sample - loss: 13.5088 - acc: 0.4996\n",
      "[13.508781677002903, 0.49962935]\n",
      "1349/1349 [==============================] - 0s 150us/sample - loss: 12.4302 - acc: 0.5004\n",
      "[12.430229992403465, 0.5003706]\n",
      "1349/1349 [==============================] - 0s 145us/sample - loss: 11.4084 - acc: 0.5011\n",
      "[11.408418304041811, 0.5011119]\n",
      "1349/1349 [==============================] - 0s 144us/sample - loss: 10.3977 - acc: 0.5019\n",
      "[10.397708692225462, 0.5018532]\n",
      "1349/1349 [==============================] - 0s 146us/sample - loss: 9.4304 - acc: 0.5041\n",
      "[9.430368106748723, 0.5040771]\n",
      "1349/1349 [==============================] - 0s 151us/sample - loss: 8.5383 - acc: 0.5093\n",
      "[8.53833956326088, 0.50926614]\n",
      "1349/1349 [==============================] - 0s 150us/sample - loss: 7.6863 - acc: 0.5145\n",
      "[7.686264467204032, 0.51445514]\n",
      "1349/1349 [==============================] - 0s 150us/sample - loss: 6.8761 - acc: 0.5226\n",
      "[6.876050753978732, 0.52260935]\n",
      "1349/1349 [==============================] - 0s 148us/sample - loss: 6.1061 - acc: 0.5308\n",
      "[6.106089412414913, 0.5307635]\n",
      "1349/1349 [==============================] - 0s 146us/sample - loss: 5.3878 - acc: 0.5515\n",
      "[5.387796718337255, 0.55151963]\n",
      "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "67/67 [==============================] - 1s 9ms/sample - loss: 5.3367 - model_1_loss: 4.6004 - model_loss: 0.6107 - model_1_acc: 0.0000e+00 - model_acc: 0.6119\n",
      "[5.336714274847686, 4.6004114, 0.610716, 0.0, 0.6119403]\n",
      "Iteration Count #####  1\n",
      "67/67 [==============================] - 0s 234us/sample - loss: 0.6850 - bacc: 0.7164 - rloss: 0.0285\n",
      "[0.6850346126663152, 0.7164179, 0.028515438]\n",
      "1349/1349 [==============================] - 0s 131us/sample - loss: 5.8865 - acc: 0.3017\n",
      "[5.8864642082098415, 0.30170497]\n",
      "1349/1349 [==============================] - 0s 149us/sample - loss: 2.5618 - acc: 0.6479\n",
      "[2.561758522637426, 0.64788735]\n",
      "67/67 [==============================] - 1s 9ms/sample - loss: 3.3031 - model_1_loss: 1.9774 - model_loss: 0.9599 - model_1_acc: 1.0000 - model_acc: 0.7612\n",
      "[3.303118584760979, 1.9773966, 0.95994633, 1.0, 0.76119405]\n",
      "Iteration Count #####  2\n",
      "67/67 [==============================] - 0s 315us/sample - loss: 0.6357 - bacc: 0.7612 - rloss: 0.0334\n",
      "[0.6356805807618953, 0.76119405, 0.033355296]\n",
      "1349/1349 [==============================] - 0s 133us/sample - loss: 2.4602 - acc: 0.6620\n",
      "[2.4602455965583636, 0.6619718]\n",
      "67/67 [==============================] - 1s 10ms/sample - loss: 6.2546 - model_1_loss: 1.9741 - model_loss: 2.9877 - model_1_acc: 1.0000 - model_acc: 0.5224\n",
      "[6.254623464683988, 1.9740924, 2.9877079, 1.0, 0.52238804]\n",
      "Iteration Count #####  3\n",
      "67/67 [==============================] - 0s 319us/sample - loss: 0.7061 - bacc: 0.7164 - rloss: 0.0385\n",
      "[0.7061293819057408, 0.7164179, 0.038452294]\n",
      "1349/1349 [==============================] - 0s 137us/sample - loss: 2.4605 - acc: 0.6620\n",
      "[2.4605313197539957, 0.6619718]\n",
      "67/67 [==============================] - 1s 11ms/sample - loss: 9.0320 - model_1_loss: 4.6101 - model_loss: 5.5859 - model_1_acc: 1.0000 - model_acc: 0.4776\n",
      "[9.032004114407211, 4.6101274, 5.585869, 1.0, 0.47761193]\n",
      "Iteration Count #####  4\n",
      "67/67 [==============================] - 0s 306us/sample - loss: 0.7221 - bacc: 0.7015 - rloss: 0.0437\n",
      "[0.7220985030950006, 0.70149255, 0.043694723]\n",
      "1349/1349 [==============================] - 0s 143us/sample - loss: 2.4602 - acc: 0.6620\n",
      "[2.4602455965583636, 0.6619718]\n",
      "67/67 [==============================] - 1s 11ms/sample - loss: 10.7866 - model_1_loss: 4.5693 - model_loss: 4.3392 - model_1_acc: 1.0000 - model_acc: 0.5224\n",
      "[10.78663694324778, 4.569275, 4.3392353, 1.0, 0.52238804]\n",
      "Iteration Count #####  5\n",
      "67/67 [==============================] - 0s 285us/sample - loss: 0.6661 - bacc: 0.7761 - rloss: 0.0482\n",
      "[0.6661448545420348, 0.7761194, 0.048171014]\n",
      "1349/1349 [==============================] - 0s 143us/sample - loss: 2.4608 - acc: 0.6620\n",
      "[2.46082574401494, 0.6619718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 12ms/sample - loss: 15.0056 - model_1_loss: 8.3727 - model_loss: 4.6315 - model_1_acc: 0.0000e+00 - model_acc: 0.5224\n",
      "[15.005644755576974, 8.372667, 4.6314726, 0.0, 0.52238804]\n",
      "Iteration Count #####  6\n",
      "67/67 [==============================] - 0s 276us/sample - loss: 0.6518 - bacc: 0.7910 - rloss: 0.0516\n",
      "[0.6518118750693193, 0.7910448, 0.05161574]\n",
      "1349/1349 [==============================] - 0s 146us/sample - loss: 3.4213 - acc: 0.4122\n",
      "[3.421347754544201, 0.41215715]\n",
      "1349/1349 [==============================] - 0s 166us/sample - loss: 1.5883 - acc: 0.5100\n",
      "[1.5882756998576086, 0.51000744]\n",
      "1349/1349 [==============================] - 0s 165us/sample - loss: 0.8410 - acc: 0.5626\n",
      "[0.8410360995180258, 0.562639]\n",
      "1349/1349 [==============================] - 0s 160us/sample - loss: 0.5658 - acc: 0.7969\n",
      "[0.5657983490323207, 0.79688656]\n",
      "67/67 [==============================] - 1s 13ms/sample - loss: 11.2394 - model_1_loss: 5.4015 - model_loss: 4.0745 - model_1_acc: 0.0000e+00 - model_acc: 0.5224\n",
      "[11.239409546353924, 5.4014897, 4.0745325, 0.0, 0.52238804]\n",
      "Iteration Count #####  7\n",
      "67/67 [==============================] - 0s 308us/sample - loss: 0.6773 - bacc: 0.7164 - rloss: 0.0555\n",
      "[0.6773032643901769, 0.7164179, 0.05551297]\n",
      "1349/1349 [==============================] - 0s 158us/sample - loss: 0.6150 - acc: 0.5745\n",
      "[0.6150306813181374, 0.5744996]\n",
      "1349/1349 [==============================] - 0s 168us/sample - loss: 0.3887 - acc: 0.8888\n",
      "[0.38870539211891064, 0.8888065]\n",
      "67/67 [==============================] - 1s 14ms/sample - loss: 6.9392 - model_1_loss: 4.9017 - model_loss: 1.9967 - model_1_acc: 1.0000 - model_acc: 0.6269\n",
      "[6.939199276824496, 4.9017377, 1.9966855, 1.0, 0.6268657]\n",
      "Iteration Count #####  8\n",
      "67/67 [==============================] - 0s 288us/sample - loss: 0.6949 - bacc: 0.6567 - rloss: 0.0613\n",
      "[0.6948814512188755, 0.6567164, 0.061328266]\n",
      "1349/1349 [==============================] - 0s 151us/sample - loss: 0.3451 - acc: 0.8888\n",
      "[0.34505752104578236, 0.8888065]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "mcp = ModelCheckpoint(\"q.hf5\", monitor='val_bacc', save_weights_only=True, save_best_only=True)\n",
    "\n",
    "class MyEarlyStopping(Callback):\n",
    "    def __init__(self, size):\n",
    "        self.baseline = size*0.5\n",
    "        super().__init__()\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.correct  = 0\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        acc = logs['acc']\n",
    "        self.correct += logs['size']*acc\n",
    "        if self.correct > self.baseline or acc > 0.7:\n",
    "            self.model.stop_training = True\n",
    "\n",
    "for i in range(100):\n",
    "    print(\"Iteration Count ##### \", i)\n",
    "    Q.fit_generator(datagen.flow(trainX, trainY, batch_size=BATCH_SIZE), epochs=100, shuffle=True, \n",
    "                    validation_data=(testX, testY), callbacks=[mcp], verbose=0)\n",
    "    \n",
    "    qres = Q.evaluate(testX, testY)\n",
    "    print(qres)\n",
    "    \n",
    "    Q.save_weights(\"qb.hf5\")\n",
    "    \n",
    "    Q.load_weights(\"q.hf5\")\n",
    "    \n",
    "\n",
    "    func = K.function(Q.input, Q.layers[-2].output)\n",
    "    Gtrain, Gval = func(trainX), func(testX)\n",
    "    \n",
    "    l = len(Gtrain)\n",
    "    \n",
    "    s = np.arange(Gtrain.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    s = s[:l//2]\n",
    "\n",
    "    fake_sp = l//2, trainX.shape[1], trainX.shape[2], trainX.shape[3]\n",
    "    \n",
    "    fake = np.random.randn(*fake_sp) \n",
    "    if i == 0:\n",
    "        fake = np.append(fake, fake, axis=0)\n",
    "    else:\n",
    "        fake = np.append(fake, K.function(gan.input, G.output)(Gtrain[s]), axis=0)\n",
    "\n",
    "    Dx = np.append(trainX, fake, axis=0)\n",
    "    Dy = np.append(np.ones((trainX.shape[0],)), np.zeros((fake.shape[0],)))\n",
    "    \n",
    "\n",
    "    dres = D.evaluate(Dx, Dy)\n",
    "    print(dres)\n",
    "\n",
    "    count = 0 \n",
    "    while dres[1] < 0.60 and count < 40:\n",
    "        s = np.arange(Dx.shape[0])\n",
    "        np.random.shuffle(s)\n",
    "        Dx, Dy = Dx[s], Dy[s]\n",
    "        D.fit(Dx, Dy, epochs=1, batch_size=32, \n",
    "              validation_data=(testX, np.ones((testX.shape[0],))), callbacks=[MyEarlyStopping(len(Dx))], verbose=0)\n",
    "        dres = D.evaluate(Dx, Dy)\n",
    "        count += 1\n",
    "        print(dres)\n",
    "    \n",
    "    gan = define_gan(G, D, Q)\n",
    "    Q.trainable = True\n",
    "    \n",
    "    gan.fit(Gtrain, [np.zeros((Gtrain.shape[0],)), trainY], batch_size=BATCH_SIZE, epochs=100, verbose=0)\n",
    "    \n",
    "    gres = gan.evaluate(Gval, [np.zeros((Gval.shape[0],)), testY])\n",
    "    \n",
    "    print(gres)\n",
    "    \n",
    "    Q.load_weights(\"qb.hf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = K.function(Q.input, Q.layers[-2].output)\n",
    "Gtrain, Gval = func(trainX), func(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func1 = K.function(G.input, G.output)\n",
    "testfake = func1(Gval)\n",
    "len(testfake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_sp = trainX[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_sp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Assignment-19.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
